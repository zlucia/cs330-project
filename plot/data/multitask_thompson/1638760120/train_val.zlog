{"score": 0.2563112605373544, "metrics": {"abstract_algebra_test": {"major": 0.26, "minor": {"acc": 0.26}}, "anatomy_test": {"major": 0.23703703703703705, "minor": {"acc": 0.23703703703703705}}, "astronomy_test": {"major": 0.26973684210526316, "minor": {"acc": 0.26973684210526316}}, "business_ethics_test": {"major": 0.25, "minor": {"acc": 0.25}}, "clinical_knowledge_test": {"major": 0.24150943396226415, "minor": {"acc": 0.24150943396226415}}, "college_biology_test": {"major": 0.2361111111111111, "minor": {"acc": 0.2361111111111111}}, "college_chemistry_test": {"major": 0.15, "minor": {"acc": 0.15}}, "college_computer_science_test": {"major": 0.24, "minor": {"acc": 0.24}}, "college_mathematics_test": {"major": 0.25, "minor": {"acc": 0.25}}, "college_medicine_test": {"major": 0.2254335260115607, "minor": {"acc": 0.2254335260115607}}, "college_physics_test": {"major": 0.3137254901960784, "minor": {"acc": 0.3137254901960784}}, "computer_security_test": {"major": 0.19, "minor": {"acc": 0.19}}, "conceptual_physics_test": {"major": 0.28085106382978725, "minor": {"acc": 0.28085106382978725}}, "econometrics_test": {"major": 0.2982456140350877, "minor": {"acc": 0.2982456140350877}}, "electrical_engineering_test": {"major": 0.31724137931034485, "minor": {"acc": 0.31724137931034485}}, "elementary_mathematics_test": {"major": 0.23015873015873015, "minor": {"acc": 0.23015873015873015}}, "formal_logic_test": {"major": 0.3333333333333333, "minor": {"acc": 0.3333333333333333}}, "global_facts_test": {"major": 0.21, "minor": {"acc": 0.21}}, "high_school_biology_test": {"major": 0.27419354838709675, "minor": {"acc": 0.27419354838709675}}, "high_school_chemistry_test": {"major": 0.27586206896551724, "minor": {"acc": 0.27586206896551724}}, "high_school_computer_science_test": {"major": 0.25, "minor": {"acc": 0.25}}, "high_school_european_history_test": {"major": 0.2787878787878788, "minor": {"acc": 0.2787878787878788}}, "high_school_geography_test": {"major": 0.17676767676767677, "minor": {"acc": 0.17676767676767677}}, "high_school_government_and_politics_test": {"major": 0.31088082901554404, "minor": {"acc": 0.31088082901554404}}, "high_school_macroeconomics_test": {"major": 0.19230769230769232, "minor": {"acc": 0.19230769230769232}}, "high_school_mathematics_test": {"major": 0.2222222222222222, "minor": {"acc": 0.2222222222222222}}, "high_school_microeconomics_test": {"major": 0.22268907563025211, "minor": {"acc": 0.22268907563025211}}, "high_school_physics_test": {"major": 0.2847682119205298, "minor": {"acc": 0.2847682119205298}}, "high_school_psychology_test": {"major": 0.28, "minor": {"acc": 0.28}}, "high_school_statistics_test": {"major": 0.2824074074074074, "minor": {"acc": 0.2824074074074074}}, "high_school_us_history_test": {"major": 0.27450980392156865, "minor": {"acc": 0.27450980392156865}}, "high_school_world_history_test": {"major": 0.15611814345991562, "minor": {"acc": 0.15611814345991562}}, "human_aging_test": {"major": 0.23318385650224216, "minor": {"acc": 0.23318385650224216}}, "human_sexuality_test": {"major": 0.2748091603053435, "minor": {"acc": 0.2748091603053435}}, "international_law_test": {"major": 0.34710743801652894, "minor": {"acc": 0.34710743801652894}}, "jurisprudence_test": {"major": 0.23148148148148148, "minor": {"acc": 0.23148148148148148}}, "logical_fallacies_test": {"major": 0.294478527607362, "minor": {"acc": 0.294478527607362}}, "machine_learning_test": {"major": 0.25, "minor": {"acc": 0.25}}, "management_test": {"major": 0.30097087378640774, "minor": {"acc": 0.30097087378640774}}, "marketing_test": {"major": 0.2863247863247863, "minor": {"acc": 0.2863247863247863}}, "medical_genetics_test": {"major": 0.23, "minor": {"acc": 0.23}}, "miscellaneous_test": {"major": 0.202, "minor": {"acc": 0.202}}, "moral_disputes_test": {"major": 0.22832369942196531, "minor": {"acc": 0.22832369942196531}}, "moral_scenarios_test": {"major": 0.234, "minor": {"acc": 0.234}}, "nutrition_test": {"major": 0.23202614379084968, "minor": {"acc": 0.23202614379084968}}, "philosophy_test": {"major": 0.24437299035369775, "minor": {"acc": 0.24437299035369775}}, "prehistory_test": {"major": 0.24691358024691357, "minor": {"acc": 0.24691358024691357}}, "professional_accounting_test": {"major": 0.25177304964539005, "minor": {"acc": 0.25177304964539005}}, "professional_law_test": {"major": 0.298, "minor": {"acc": 0.298}}, "professional_medicine_test": {"major": 0.25735294117647056, "minor": {"acc": 0.25735294117647056}}, "professional_psychology_test": {"major": 0.25, "minor": {"acc": 0.25}}, "public_relations_test": {"major": 0.22727272727272727, "minor": {"acc": 0.22727272727272727}}, "security_studies_test": {"major": 0.2653061224489796, "minor": {"acc": 0.2653061224489796}}, "sociology_test": {"major": 0.31840796019900497, "minor": {"acc": 0.31840796019900497}}, "us_foreign_policy_test": {"major": 0.25, "minor": {"acc": 0.25}}, "virology_test": {"major": 0.3132530120481928, "minor": {"acc": 0.3132530120481928}}, "world_religions_test": {"major": 0.32748538011695905, "minor": {"acc": 0.32748538011695905}}}, "train_state": {"global_steps": 29, "task_steps": {"abstract_algebra_test": 2, "anatomy_test": 1, "astronomy_test": 1, "business_ethics_test": 0, "clinical_knowledge_test": 0, "college_biology_test": 1, "college_chemistry_test": 1, "college_computer_science_test": 0, "college_mathematics_test": 1, "college_medicine_test": 0, "college_physics_test": 1, "computer_security_test": 0, "conceptual_physics_test": 0, "econometrics_test": 0, "electrical_engineering_test": 1, "elementary_mathematics_test": 0, "formal_logic_test": 1, "global_facts_test": 0, "high_school_biology_test": 1, "high_school_chemistry_test": 0, "high_school_computer_science_test": 1, "high_school_european_history_test": 2, "high_school_geography_test": 0, "high_school_government_and_politics_test": 2, "high_school_macroeconomics_test": 1, "high_school_mathematics_test": 1, "high_school_microeconomics_test": 0, "high_school_physics_test": 0, "high_school_psychology_test": 0, "high_school_statistics_test": 0, "high_school_us_history_test": 0, "high_school_world_history_test": 0, "human_aging_test": 0, "human_sexuality_test": 2, "international_law_test": 1, "jurisprudence_test": 0, "logical_fallacies_test": 0, "machine_learning_test": 0, "management_test": 0, "marketing_test": 0, "medical_genetics_test": 1, "miscellaneous_test": 0, "moral_disputes_test": 0, "moral_scenarios_test": 0, "nutrition_test": 0, "philosophy_test": 0, "prehistory_test": 0, "professional_accounting_test": 1, "professional_law_test": 1, "professional_medicine_test": 0, "professional_psychology_test": 0, "public_relations_test": 0, "security_studies_test": 1, "sociology_test": 1, "us_foreign_policy_test": 1, "virology_test": 0, "world_religions_test": 2}}, "TIMESTAMP": 1638760312.2873297}
{"score": 0.25907039997868336, "metrics": {"abstract_algebra_test": {"major": 0.23, "minor": {"acc": 0.23}}, "anatomy_test": {"major": 0.28888888888888886, "minor": {"acc": 0.28888888888888886}}, "astronomy_test": {"major": 0.3092105263157895, "minor": {"acc": 0.3092105263157895}}, "business_ethics_test": {"major": 0.34, "minor": {"acc": 0.34}}, "clinical_knowledge_test": {"major": 0.2641509433962264, "minor": {"acc": 0.2641509433962264}}, "college_biology_test": {"major": 0.24305555555555555, "minor": {"acc": 0.24305555555555555}}, "college_chemistry_test": {"major": 0.2, "minor": {"acc": 0.2}}, "college_computer_science_test": {"major": 0.17, "minor": {"acc": 0.17}}, "college_mathematics_test": {"major": 0.32, "minor": {"acc": 0.32}}, "college_medicine_test": {"major": 0.23121387283236994, "minor": {"acc": 0.23121387283236994}}, "college_physics_test": {"major": 0.27450980392156865, "minor": {"acc": 0.27450980392156865}}, "computer_security_test": {"major": 0.21, "minor": {"acc": 0.21}}, "conceptual_physics_test": {"major": 0.28085106382978725, "minor": {"acc": 0.28085106382978725}}, "econometrics_test": {"major": 0.2982456140350877, "minor": {"acc": 0.2982456140350877}}, "electrical_engineering_test": {"major": 0.25517241379310346, "minor": {"acc": 0.25517241379310346}}, "elementary_mathematics_test": {"major": 0.22486772486772486, "minor": {"acc": 0.22486772486772486}}, "formal_logic_test": {"major": 0.2857142857142857, "minor": {"acc": 0.2857142857142857}}, "global_facts_test": {"major": 0.28, "minor": {"acc": 0.28}}, "high_school_biology_test": {"major": 0.25483870967741934, "minor": {"acc": 0.25483870967741934}}, "high_school_chemistry_test": {"major": 0.26108374384236455, "minor": {"acc": 0.26108374384236455}}, "high_school_computer_science_test": {"major": 0.21, "minor": {"acc": 0.21}}, "high_school_european_history_test": {"major": 0.16363636363636364, "minor": {"acc": 0.16363636363636364}}, "high_school_geography_test": {"major": 0.23232323232323232, "minor": {"acc": 0.23232323232323232}}, "high_school_government_and_politics_test": {"major": 0.31088082901554404, "minor": {"acc": 0.31088082901554404}}, "high_school_macroeconomics_test": {"major": 0.21025641025641026, "minor": {"acc": 0.21025641025641026}}, "high_school_mathematics_test": {"major": 0.2037037037037037, "minor": {"acc": 0.2037037037037037}}, "high_school_microeconomics_test": {"major": 0.25630252100840334, "minor": {"acc": 0.25630252100840334}}, "high_school_physics_test": {"major": 0.25165562913907286, "minor": {"acc": 0.25165562913907286}}, "high_school_psychology_test": {"major": 0.28, "minor": {"acc": 0.28}}, "high_school_statistics_test": {"major": 0.3148148148148148, "minor": {"acc": 0.3148148148148148}}, "high_school_us_history_test": {"major": 0.25980392156862747, "minor": {"acc": 0.25980392156862747}}, "high_school_world_history_test": {"major": 0.1518987341772152, "minor": {"acc": 0.1518987341772152}}, "human_aging_test": {"major": 0.23318385650224216, "minor": {"acc": 0.23318385650224216}}, "human_sexuality_test": {"major": 0.2824427480916031, "minor": {"acc": 0.2824427480916031}}, "international_law_test": {"major": 0.3140495867768595, "minor": {"acc": 0.3140495867768595}}, "jurisprudence_test": {"major": 0.17592592592592593, "minor": {"acc": 0.17592592592592593}}, "logical_fallacies_test": {"major": 0.31901840490797545, "minor": {"acc": 0.31901840490797545}}, "machine_learning_test": {"major": 0.2767857142857143, "minor": {"acc": 0.2767857142857143}}, "management_test": {"major": 0.2621359223300971, "minor": {"acc": 0.2621359223300971}}, "marketing_test": {"major": 0.45726495726495725, "minor": {"acc": 0.45726495726495725}}, "medical_genetics_test": {"major": 0.26, "minor": {"acc": 0.26}}, "miscellaneous_test": {"major": 0.308, "minor": {"acc": 0.308}}, "moral_disputes_test": {"major": 0.24566473988439305, "minor": {"acc": 0.24566473988439305}}, "moral_scenarios_test": {"major": 0.252, "minor": {"acc": 0.252}}, "nutrition_test": {"major": 0.2679738562091503, "minor": {"acc": 0.2679738562091503}}, "philosophy_test": {"major": 0.2508038585209003, "minor": {"acc": 0.2508038585209003}}, "prehistory_test": {"major": 0.2191358024691358, "minor": {"acc": 0.2191358024691358}}, "professional_accounting_test": {"major": 0.2375886524822695, "minor": {"acc": 0.2375886524822695}}, "professional_law_test": {"major": 0.29, "minor": {"acc": 0.29}}, "professional_medicine_test": {"major": 0.2867647058823529, "minor": {"acc": 0.2867647058823529}}, "professional_psychology_test": {"major": 0.24, "minor": {"acc": 0.24}}, "public_relations_test": {"major": 0.23636363636363636, "minor": {"acc": 0.23636363636363636}}, "security_studies_test": {"major": 0.22448979591836735, "minor": {"acc": 0.22448979591836735}}, "sociology_test": {"major": 0.2935323383084577, "minor": {"acc": 0.2935323383084577}}, "us_foreign_policy_test": {"major": 0.24, "minor": {"acc": 0.24}}, "virology_test": {"major": 0.21686746987951808, "minor": {"acc": 0.21686746987951808}}, "world_religions_test": {"major": 0.30994152046783624, "minor": {"acc": 0.30994152046783624}}}, "train_state": {"global_steps": 59, "task_steps": {"abstract_algebra_test": 2, "anatomy_test": 2, "astronomy_test": 1, "business_ethics_test": 1, "clinical_knowledge_test": 2, "college_biology_test": 2, "college_chemistry_test": 2, "college_computer_science_test": 1, "college_mathematics_test": 1, "college_medicine_test": 0, "college_physics_test": 1, "computer_security_test": 0, "conceptual_physics_test": 0, "econometrics_test": 0, "electrical_engineering_test": 1, "elementary_mathematics_test": 1, "formal_logic_test": 2, "global_facts_test": 0, "high_school_biology_test": 1, "high_school_chemistry_test": 1, "high_school_computer_science_test": 1, "high_school_european_history_test": 2, "high_school_geography_test": 0, "high_school_government_and_politics_test": 3, "high_school_macroeconomics_test": 2, "high_school_mathematics_test": 1, "high_school_microeconomics_test": 1, "high_school_physics_test": 0, "high_school_psychology_test": 0, "high_school_statistics_test": 0, "high_school_us_history_test": 1, "high_school_world_history_test": 2, "human_aging_test": 0, "human_sexuality_test": 3, "international_law_test": 1, "jurisprudence_test": 1, "logical_fallacies_test": 0, "machine_learning_test": 2, "management_test": 1, "marketing_test": 1, "medical_genetics_test": 1, "miscellaneous_test": 1, "moral_disputes_test": 0, "moral_scenarios_test": 1, "nutrition_test": 0, "philosophy_test": 1, "prehistory_test": 0, "professional_accounting_test": 1, "professional_law_test": 2, "professional_medicine_test": 1, "professional_psychology_test": 0, "public_relations_test": 0, "security_studies_test": 1, "sociology_test": 2, "us_foreign_policy_test": 3, "virology_test": 0, "world_religions_test": 2}}, "TIMESTAMP": 1638760532.546272}
{"score": 0.2611657019894215, "metrics": {"abstract_algebra_test": {"major": 0.29, "minor": {"acc": 0.29}}, "anatomy_test": {"major": 0.22962962962962963, "minor": {"acc": 0.22962962962962963}}, "astronomy_test": {"major": 0.2894736842105263, "minor": {"acc": 0.2894736842105263}}, "business_ethics_test": {"major": 0.38, "minor": {"acc": 0.38}}, "clinical_knowledge_test": {"major": 0.2679245283018868, "minor": {"acc": 0.2679245283018868}}, "college_biology_test": {"major": 0.24305555555555555, "minor": {"acc": 0.24305555555555555}}, "college_chemistry_test": {"major": 0.22, "minor": {"acc": 0.22}}, "college_computer_science_test": {"major": 0.24, "minor": {"acc": 0.24}}, "college_mathematics_test": {"major": 0.24, "minor": {"acc": 0.24}}, "college_medicine_test": {"major": 0.21965317919075145, "minor": {"acc": 0.21965317919075145}}, "college_physics_test": {"major": 0.3333333333333333, "minor": {"acc": 0.3333333333333333}}, "computer_security_test": {"major": 0.32, "minor": {"acc": 0.32}}, "conceptual_physics_test": {"major": 0.2978723404255319, "minor": {"acc": 0.2978723404255319}}, "econometrics_test": {"major": 0.2894736842105263, "minor": {"acc": 0.2894736842105263}}, "electrical_engineering_test": {"major": 0.2620689655172414, "minor": {"acc": 0.2620689655172414}}, "elementary_mathematics_test": {"major": 0.21164021164021163, "minor": {"acc": 0.21164021164021163}}, "formal_logic_test": {"major": 0.2857142857142857, "minor": {"acc": 0.2857142857142857}}, "global_facts_test": {"major": 0.22, "minor": {"acc": 0.22}}, "high_school_biology_test": {"major": 0.25161290322580643, "minor": {"acc": 0.25161290322580643}}, "high_school_chemistry_test": {"major": 0.23645320197044334, "minor": {"acc": 0.23645320197044334}}, "high_school_computer_science_test": {"major": 0.22, "minor": {"acc": 0.22}}, "high_school_european_history_test": {"major": 0.16363636363636364, "minor": {"acc": 0.16363636363636364}}, "high_school_geography_test": {"major": 0.3686868686868687, "minor": {"acc": 0.3686868686868687}}, "high_school_government_and_politics_test": {"major": 0.29015544041450775, "minor": {"acc": 0.29015544041450775}}, "high_school_macroeconomics_test": {"major": 0.2153846153846154, "minor": {"acc": 0.2153846153846154}}, "high_school_mathematics_test": {"major": 0.2074074074074074, "minor": {"acc": 0.2074074074074074}}, "high_school_microeconomics_test": {"major": 0.23109243697478993, "minor": {"acc": 0.23109243697478993}}, "high_school_physics_test": {"major": 0.2251655629139073, "minor": {"acc": 0.2251655629139073}}, "high_school_psychology_test": {"major": 0.28, "minor": {"acc": 0.28}}, "high_school_statistics_test": {"major": 0.2638888888888889, "minor": {"acc": 0.2638888888888889}}, "high_school_us_history_test": {"major": 0.29901960784313725, "minor": {"acc": 0.29901960784313725}}, "high_school_world_history_test": {"major": 0.189873417721519, "minor": {"acc": 0.189873417721519}}, "human_aging_test": {"major": 0.28699551569506726, "minor": {"acc": 0.28699551569506726}}, "human_sexuality_test": {"major": 0.2366412213740458, "minor": {"acc": 0.2366412213740458}}, "international_law_test": {"major": 0.32231404958677684, "minor": {"acc": 0.32231404958677684}}, "jurisprudence_test": {"major": 0.2037037037037037, "minor": {"acc": 0.2037037037037037}}, "logical_fallacies_test": {"major": 0.3619631901840491, "minor": {"acc": 0.3619631901840491}}, "machine_learning_test": {"major": 0.26785714285714285, "minor": {"acc": 0.26785714285714285}}, "management_test": {"major": 0.27184466019417475, "minor": {"acc": 0.27184466019417475}}, "marketing_test": {"major": 0.49572649572649574, "minor": {"acc": 0.49572649572649574}}, "medical_genetics_test": {"major": 0.22, "minor": {"acc": 0.22}}, "miscellaneous_test": {"major": 0.306, "minor": {"acc": 0.306}}, "moral_disputes_test": {"major": 0.24566473988439305, "minor": {"acc": 0.24566473988439305}}, "moral_scenarios_test": {"major": 0.256, "minor": {"acc": 0.256}}, "nutrition_test": {"major": 0.2222222222222222, "minor": {"acc": 0.2222222222222222}}, "philosophy_test": {"major": 0.24115755627009647, "minor": {"acc": 0.24115755627009647}}, "prehistory_test": {"major": 0.25, "minor": {"acc": 0.25}}, "professional_accounting_test": {"major": 0.26595744680851063, "minor": {"acc": 0.26595744680851063}}, "professional_law_test": {"major": 0.262, "minor": {"acc": 0.262}}, "professional_medicine_test": {"major": 0.28308823529411764, "minor": {"acc": 0.28308823529411764}}, "professional_psychology_test": {"major": 0.242, "minor": {"acc": 0.242}}, "public_relations_test": {"major": 0.2, "minor": {"acc": 0.2}}, "security_studies_test": {"major": 0.2163265306122449, "minor": {"acc": 0.2163265306122449}}, "sociology_test": {"major": 0.2885572139303483, "minor": {"acc": 0.2885572139303483}}, "us_foreign_policy_test": {"major": 0.22, "minor": {"acc": 0.22}}, "virology_test": {"major": 0.18674698795180722, "minor": {"acc": 0.18674698795180722}}, "world_religions_test": {"major": 0.25146198830409355, "minor": {"acc": 0.25146198830409355}}}, "train_state": {"global_steps": 89, "task_steps": {"abstract_algebra_test": 2, "anatomy_test": 2, "astronomy_test": 1, "business_ethics_test": 1, "clinical_knowledge_test": 2, "college_biology_test": 2, "college_chemistry_test": 2, "college_computer_science_test": 2, "college_mathematics_test": 1, "college_medicine_test": 1, "college_physics_test": 1, "computer_security_test": 1, "conceptual_physics_test": 1, "econometrics_test": 2, "electrical_engineering_test": 1, "elementary_mathematics_test": 2, "formal_logic_test": 2, "global_facts_test": 0, "high_school_biology_test": 2, "high_school_chemistry_test": 1, "high_school_computer_science_test": 3, "high_school_european_history_test": 2, "high_school_geography_test": 1, "high_school_government_and_politics_test": 3, "high_school_macroeconomics_test": 2, "high_school_mathematics_test": 2, "high_school_microeconomics_test": 2, "high_school_physics_test": 1, "high_school_psychology_test": 0, "high_school_statistics_test": 2, "high_school_us_history_test": 1, "high_school_world_history_test": 2, "human_aging_test": 2, "human_sexuality_test": 3, "international_law_test": 1, "jurisprudence_test": 1, "logical_fallacies_test": 1, "machine_learning_test": 2, "management_test": 2, "marketing_test": 2, "medical_genetics_test": 1, "miscellaneous_test": 1, "moral_disputes_test": 2, "moral_scenarios_test": 2, "nutrition_test": 2, "philosophy_test": 1, "prehistory_test": 2, "professional_accounting_test": 2, "professional_law_test": 2, "professional_medicine_test": 1, "professional_psychology_test": 0, "public_relations_test": 0, "security_studies_test": 1, "sociology_test": 2, "us_foreign_policy_test": 3, "virology_test": 0, "world_religions_test": 3}}, "TIMESTAMP": 1638760695.4453833}
{"score": 0.2609056852624105, "metrics": {"abstract_algebra_test": {"major": 0.23, "minor": {"acc": 0.23}}, "anatomy_test": {"major": 0.2, "minor": {"acc": 0.2}}, "astronomy_test": {"major": 0.3092105263157895, "minor": {"acc": 0.3092105263157895}}, "business_ethics_test": {"major": 0.35, "minor": {"acc": 0.35}}, "clinical_knowledge_test": {"major": 0.2792452830188679, "minor": {"acc": 0.2792452830188679}}, "college_biology_test": {"major": 0.22916666666666666, "minor": {"acc": 0.22916666666666666}}, "college_chemistry_test": {"major": 0.23, "minor": {"acc": 0.23}}, "college_computer_science_test": {"major": 0.25, "minor": {"acc": 0.25}}, "college_mathematics_test": {"major": 0.23, "minor": {"acc": 0.23}}, "college_medicine_test": {"major": 0.2138728323699422, "minor": {"acc": 0.2138728323699422}}, "college_physics_test": {"major": 0.37254901960784315, "minor": {"acc": 0.37254901960784315}}, "computer_security_test": {"major": 0.28, "minor": {"acc": 0.28}}, "conceptual_physics_test": {"major": 0.2765957446808511, "minor": {"acc": 0.2765957446808511}}, "econometrics_test": {"major": 0.2719298245614035, "minor": {"acc": 0.2719298245614035}}, "electrical_engineering_test": {"major": 0.2413793103448276, "minor": {"acc": 0.2413793103448276}}, "elementary_mathematics_test": {"major": 0.21957671957671956, "minor": {"acc": 0.21957671957671956}}, "formal_logic_test": {"major": 0.2857142857142857, "minor": {"acc": 0.2857142857142857}}, "global_facts_test": {"major": 0.25, "minor": {"acc": 0.25}}, "high_school_biology_test": {"major": 0.2645161290322581, "minor": {"acc": 0.2645161290322581}}, "high_school_chemistry_test": {"major": 0.22660098522167488, "minor": {"acc": 0.22660098522167488}}, "high_school_computer_science_test": {"major": 0.21, "minor": {"acc": 0.21}}, "high_school_european_history_test": {"major": 0.1696969696969697, "minor": {"acc": 0.1696969696969697}}, "high_school_geography_test": {"major": 0.35858585858585856, "minor": {"acc": 0.35858585858585856}}, "high_school_government_and_politics_test": {"major": 0.27979274611398963, "minor": {"acc": 0.27979274611398963}}, "high_school_macroeconomics_test": {"major": 0.20256410256410257, "minor": {"acc": 0.20256410256410257}}, "high_school_mathematics_test": {"major": 0.22962962962962963, "minor": {"acc": 0.22962962962962963}}, "high_school_microeconomics_test": {"major": 0.25630252100840334, "minor": {"acc": 0.25630252100840334}}, "high_school_physics_test": {"major": 0.2251655629139073, "minor": {"acc": 0.2251655629139073}}, "high_school_psychology_test": {"major": 0.236, "minor": {"acc": 0.236}}, "high_school_statistics_test": {"major": 0.24537037037037038, "minor": {"acc": 0.24537037037037038}}, "high_school_us_history_test": {"major": 0.3382352941176471, "minor": {"acc": 0.3382352941176471}}, "high_school_world_history_test": {"major": 0.19831223628691982, "minor": {"acc": 0.19831223628691982}}, "human_aging_test": {"major": 0.28699551569506726, "minor": {"acc": 0.28699551569506726}}, "human_sexuality_test": {"major": 0.1984732824427481, "minor": {"acc": 0.1984732824427481}}, "international_law_test": {"major": 0.30578512396694213, "minor": {"acc": 0.30578512396694213}}, "jurisprudence_test": {"major": 0.24074074074074073, "minor": {"acc": 0.24074074074074073}}, "logical_fallacies_test": {"major": 0.3558282208588957, "minor": {"acc": 0.3558282208588957}}, "machine_learning_test": {"major": 0.22321428571428573, "minor": {"acc": 0.22321428571428573}}, "management_test": {"major": 0.3300970873786408, "minor": {"acc": 0.3300970873786408}}, "marketing_test": {"major": 0.48717948717948717, "minor": {"acc": 0.48717948717948717}}, "medical_genetics_test": {"major": 0.25, "minor": {"acc": 0.25}}, "miscellaneous_test": {"major": 0.29, "minor": {"acc": 0.29}}, "moral_disputes_test": {"major": 0.2543352601156069, "minor": {"acc": 0.2543352601156069}}, "moral_scenarios_test": {"major": 0.252, "minor": {"acc": 0.252}}, "nutrition_test": {"major": 0.21895424836601307, "minor": {"acc": 0.21895424836601307}}, "philosophy_test": {"major": 0.26366559485530544, "minor": {"acc": 0.26366559485530544}}, "prehistory_test": {"major": 0.2345679012345679, "minor": {"acc": 0.2345679012345679}}, "professional_accounting_test": {"major": 0.25886524822695034, "minor": {"acc": 0.25886524822695034}}, "professional_law_test": {"major": 0.24, "minor": {"acc": 0.24}}, "professional_medicine_test": {"major": 0.29411764705882354, "minor": {"acc": 0.29411764705882354}}, "professional_psychology_test": {"major": 0.26, "minor": {"acc": 0.26}}, "public_relations_test": {"major": 0.22727272727272727, "minor": {"acc": 0.22727272727272727}}, "security_studies_test": {"major": 0.23673469387755103, "minor": {"acc": 0.23673469387755103}}, "sociology_test": {"major": 0.27860696517412936, "minor": {"acc": 0.27860696517412936}}, "us_foreign_policy_test": {"major": 0.25, "minor": {"acc": 0.25}}, "virology_test": {"major": 0.21686746987951808, "minor": {"acc": 0.21686746987951808}}, "world_religions_test": {"major": 0.2573099415204678, "minor": {"acc": 0.2573099415204678}}}, "train_state": {"global_steps": 119, "task_steps": {"abstract_algebra_test": 3, "anatomy_test": 2, "astronomy_test": 2, "business_ethics_test": 3, "clinical_knowledge_test": 2, "college_biology_test": 2, "college_chemistry_test": 2, "college_computer_science_test": 2, "college_mathematics_test": 2, "college_medicine_test": 3, "college_physics_test": 1, "computer_security_test": 1, "conceptual_physics_test": 2, "econometrics_test": 2, "electrical_engineering_test": 1, "elementary_mathematics_test": 2, "formal_logic_test": 2, "global_facts_test": 0, "high_school_biology_test": 2, "high_school_chemistry_test": 2, "high_school_computer_science_test": 3, "high_school_european_history_test": 2, "high_school_geography_test": 2, "high_school_government_and_politics_test": 3, "high_school_macroeconomics_test": 3, "high_school_mathematics_test": 2, "high_school_microeconomics_test": 3, "high_school_physics_test": 3, "high_school_psychology_test": 2, "high_school_statistics_test": 3, "high_school_us_history_test": 2, "high_school_world_history_test": 2, "human_aging_test": 2, "human_sexuality_test": 3, "international_law_test": 1, "jurisprudence_test": 2, "logical_fallacies_test": 3, "machine_learning_test": 2, "management_test": 2, "marketing_test": 2, "medical_genetics_test": 2, "miscellaneous_test": 2, "moral_disputes_test": 2, "moral_scenarios_test": 2, "nutrition_test": 3, "philosophy_test": 1, "prehistory_test": 2, "professional_accounting_test": 2, "professional_law_test": 2, "professional_medicine_test": 2, "professional_psychology_test": 1, "public_relations_test": 0, "security_studies_test": 2, "sociology_test": 2, "us_foreign_policy_test": 3, "virology_test": 2, "world_religions_test": 4}}, "TIMESTAMP": 1638760862.1420255}
{"score": 0.2607560228774004, "metrics": {"abstract_algebra_test": {"major": 0.22, "minor": {"acc": 0.22}}, "anatomy_test": {"major": 0.2074074074074074, "minor": {"acc": 0.2074074074074074}}, "astronomy_test": {"major": 0.29605263157894735, "minor": {"acc": 0.29605263157894735}}, "business_ethics_test": {"major": 0.33, "minor": {"acc": 0.33}}, "clinical_knowledge_test": {"major": 0.2641509433962264, "minor": {"acc": 0.2641509433962264}}, "college_biology_test": {"major": 0.2152777777777778, "minor": {"acc": 0.2152777777777778}}, "college_chemistry_test": {"major": 0.29, "minor": {"acc": 0.29}}, "college_computer_science_test": {"major": 0.27, "minor": {"acc": 0.27}}, "college_mathematics_test": {"major": 0.23, "minor": {"acc": 0.23}}, "college_medicine_test": {"major": 0.21965317919075145, "minor": {"acc": 0.21965317919075145}}, "college_physics_test": {"major": 0.3333333333333333, "minor": {"acc": 0.3333333333333333}}, "computer_security_test": {"major": 0.27, "minor": {"acc": 0.27}}, "conceptual_physics_test": {"major": 0.2851063829787234, "minor": {"acc": 0.2851063829787234}}, "econometrics_test": {"major": 0.23684210526315788, "minor": {"acc": 0.23684210526315788}}, "electrical_engineering_test": {"major": 0.25517241379310346, "minor": {"acc": 0.25517241379310346}}, "elementary_mathematics_test": {"major": 0.2037037037037037, "minor": {"acc": 0.2037037037037037}}, "formal_logic_test": {"major": 0.2777777777777778, "minor": {"acc": 0.2777777777777778}}, "global_facts_test": {"major": 0.3, "minor": {"acc": 0.3}}, "high_school_biology_test": {"major": 0.24516129032258063, "minor": {"acc": 0.24516129032258063}}, "high_school_chemistry_test": {"major": 0.24630541871921183, "minor": {"acc": 0.24630541871921183}}, "high_school_computer_science_test": {"major": 0.21, "minor": {"acc": 0.21}}, "high_school_european_history_test": {"major": 0.16363636363636364, "minor": {"acc": 0.16363636363636364}}, "high_school_geography_test": {"major": 0.36363636363636365, "minor": {"acc": 0.36363636363636365}}, "high_school_government_and_politics_test": {"major": 0.30569948186528495, "minor": {"acc": 0.30569948186528495}}, "high_school_macroeconomics_test": {"major": 0.2153846153846154, "minor": {"acc": 0.2153846153846154}}, "high_school_mathematics_test": {"major": 0.17037037037037037, "minor": {"acc": 0.17037037037037037}}, "high_school_microeconomics_test": {"major": 0.2605042016806723, "minor": {"acc": 0.2605042016806723}}, "high_school_physics_test": {"major": 0.2185430463576159, "minor": {"acc": 0.2185430463576159}}, "high_school_psychology_test": {"major": 0.232, "minor": {"acc": 0.232}}, "high_school_statistics_test": {"major": 0.25462962962962965, "minor": {"acc": 0.25462962962962965}}, "high_school_us_history_test": {"major": 0.35294117647058826, "minor": {"acc": 0.35294117647058826}}, "high_school_world_history_test": {"major": 0.16455696202531644, "minor": {"acc": 0.16455696202531644}}, "human_aging_test": {"major": 0.2600896860986547, "minor": {"acc": 0.2600896860986547}}, "human_sexuality_test": {"major": 0.22900763358778625, "minor": {"acc": 0.22900763358778625}}, "international_law_test": {"major": 0.3140495867768595, "minor": {"acc": 0.3140495867768595}}, "jurisprudence_test": {"major": 0.21296296296296297, "minor": {"acc": 0.21296296296296297}}, "logical_fallacies_test": {"major": 0.39263803680981596, "minor": {"acc": 0.39263803680981596}}, "machine_learning_test": {"major": 0.2767857142857143, "minor": {"acc": 0.2767857142857143}}, "management_test": {"major": 0.2912621359223301, "minor": {"acc": 0.2912621359223301}}, "marketing_test": {"major": 0.47863247863247865, "minor": {"acc": 0.47863247863247865}}, "medical_genetics_test": {"major": 0.22, "minor": {"acc": 0.22}}, "miscellaneous_test": {"major": 0.292, "minor": {"acc": 0.292}}, "moral_disputes_test": {"major": 0.2658959537572254, "minor": {"acc": 0.2658959537572254}}, "moral_scenarios_test": {"major": 0.26, "minor": {"acc": 0.26}}, "nutrition_test": {"major": 0.24183006535947713, "minor": {"acc": 0.24183006535947713}}, "philosophy_test": {"major": 0.2540192926045016, "minor": {"acc": 0.2540192926045016}}, "prehistory_test": {"major": 0.24382716049382716, "minor": {"acc": 0.24382716049382716}}, "professional_accounting_test": {"major": 0.2553191489361702, "minor": {"acc": 0.2553191489361702}}, "professional_law_test": {"major": 0.25, "minor": {"acc": 0.25}}, "professional_medicine_test": {"major": 0.29044117647058826, "minor": {"acc": 0.29044117647058826}}, "professional_psychology_test": {"major": 0.248, "minor": {"acc": 0.248}}, "public_relations_test": {"major": 0.24545454545454545, "minor": {"acc": 0.24545454545454545}}, "security_studies_test": {"major": 0.2163265306122449, "minor": {"acc": 0.2163265306122449}}, "sociology_test": {"major": 0.27860696517412936, "minor": {"acc": 0.27860696517412936}}, "us_foreign_policy_test": {"major": 0.24, "minor": {"acc": 0.24}}, "virology_test": {"major": 0.23493975903614459, "minor": {"acc": 0.23493975903614459}}, "world_religions_test": {"major": 0.2631578947368421, "minor": {"acc": 0.2631578947368421}}}, "train_state": {"global_steps": 149, "task_steps": {"abstract_algebra_test": 3, "anatomy_test": 2, "astronomy_test": 3, "business_ethics_test": 3, "clinical_knowledge_test": 3, "college_biology_test": 2, "college_chemistry_test": 2, "college_computer_science_test": 2, "college_mathematics_test": 2, "college_medicine_test": 3, "college_physics_test": 2, "computer_security_test": 2, "conceptual_physics_test": 2, "econometrics_test": 4, "electrical_engineering_test": 3, "elementary_mathematics_test": 3, "formal_logic_test": 3, "global_facts_test": 1, "high_school_biology_test": 3, "high_school_chemistry_test": 3, "high_school_computer_science_test": 3, "high_school_european_history_test": 3, "high_school_geography_test": 3, "high_school_government_and_politics_test": 3, "high_school_macroeconomics_test": 3, "high_school_mathematics_test": 2, "high_school_microeconomics_test": 3, "high_school_physics_test": 3, "high_school_psychology_test": 3, "high_school_statistics_test": 3, "high_school_us_history_test": 2, "high_school_world_history_test": 2, "human_aging_test": 3, "human_sexuality_test": 3, "international_law_test": 2, "jurisprudence_test": 2, "logical_fallacies_test": 3, "machine_learning_test": 4, "management_test": 2, "marketing_test": 3, "medical_genetics_test": 2, "miscellaneous_test": 2, "moral_disputes_test": 3, "moral_scenarios_test": 2, "nutrition_test": 3, "philosophy_test": 2, "prehistory_test": 2, "professional_accounting_test": 3, "professional_law_test": 2, "professional_medicine_test": 3, "professional_psychology_test": 3, "public_relations_test": 2, "security_studies_test": 2, "sociology_test": 3, "us_foreign_policy_test": 3, "virology_test": 2, "world_religions_test": 4}}, "TIMESTAMP": 1638761039.9021716}
{"score": 0.25858020745608984, "metrics": {"abstract_algebra_test": {"major": 0.19, "minor": {"acc": 0.19}}, "anatomy_test": {"major": 0.21481481481481482, "minor": {"acc": 0.21481481481481482}}, "astronomy_test": {"major": 0.28289473684210525, "minor": {"acc": 0.28289473684210525}}, "business_ethics_test": {"major": 0.36, "minor": {"acc": 0.36}}, "clinical_knowledge_test": {"major": 0.27169811320754716, "minor": {"acc": 0.27169811320754716}}, "college_biology_test": {"major": 0.2222222222222222, "minor": {"acc": 0.2222222222222222}}, "college_chemistry_test": {"major": 0.27, "minor": {"acc": 0.27}}, "college_computer_science_test": {"major": 0.25, "minor": {"acc": 0.25}}, "college_mathematics_test": {"major": 0.18, "minor": {"acc": 0.18}}, "college_medicine_test": {"major": 0.21965317919075145, "minor": {"acc": 0.21965317919075145}}, "college_physics_test": {"major": 0.30392156862745096, "minor": {"acc": 0.30392156862745096}}, "computer_security_test": {"major": 0.26, "minor": {"acc": 0.26}}, "conceptual_physics_test": {"major": 0.2978723404255319, "minor": {"acc": 0.2978723404255319}}, "econometrics_test": {"major": 0.2543859649122807, "minor": {"acc": 0.2543859649122807}}, "electrical_engineering_test": {"major": 0.2413793103448276, "minor": {"acc": 0.2413793103448276}}, "elementary_mathematics_test": {"major": 0.2037037037037037, "minor": {"acc": 0.2037037037037037}}, "formal_logic_test": {"major": 0.2619047619047619, "minor": {"acc": 0.2619047619047619}}, "global_facts_test": {"major": 0.3, "minor": {"acc": 0.3}}, "high_school_biology_test": {"major": 0.24193548387096775, "minor": {"acc": 0.24193548387096775}}, "high_school_chemistry_test": {"major": 0.22660098522167488, "minor": {"acc": 0.22660098522167488}}, "high_school_computer_science_test": {"major": 0.21, "minor": {"acc": 0.21}}, "high_school_european_history_test": {"major": 0.16363636363636364, "minor": {"acc": 0.16363636363636364}}, "high_school_geography_test": {"major": 0.37373737373737376, "minor": {"acc": 0.37373737373737376}}, "high_school_government_and_politics_test": {"major": 0.32124352331606215, "minor": {"acc": 0.32124352331606215}}, "high_school_macroeconomics_test": {"major": 0.2076923076923077, "minor": {"acc": 0.2076923076923077}}, "high_school_mathematics_test": {"major": 0.1925925925925926, "minor": {"acc": 0.1925925925925926}}, "high_school_microeconomics_test": {"major": 0.2647058823529412, "minor": {"acc": 0.2647058823529412}}, "high_school_physics_test": {"major": 0.2251655629139073, "minor": {"acc": 0.2251655629139073}}, "high_school_psychology_test": {"major": 0.25, "minor": {"acc": 0.25}}, "high_school_statistics_test": {"major": 0.24074074074074073, "minor": {"acc": 0.24074074074074073}}, "high_school_us_history_test": {"major": 0.3431372549019608, "minor": {"acc": 0.3431372549019608}}, "high_school_world_history_test": {"major": 0.189873417721519, "minor": {"acc": 0.189873417721519}}, "human_aging_test": {"major": 0.2645739910313901, "minor": {"acc": 0.2645739910313901}}, "human_sexuality_test": {"major": 0.2366412213740458, "minor": {"acc": 0.2366412213740458}}, "international_law_test": {"major": 0.3140495867768595, "minor": {"acc": 0.3140495867768595}}, "jurisprudence_test": {"major": 0.2222222222222222, "minor": {"acc": 0.2222222222222222}}, "logical_fallacies_test": {"major": 0.3803680981595092, "minor": {"acc": 0.3803680981595092}}, "machine_learning_test": {"major": 0.26785714285714285, "minor": {"acc": 0.26785714285714285}}, "management_test": {"major": 0.2912621359223301, "minor": {"acc": 0.2912621359223301}}, "marketing_test": {"major": 0.47863247863247865, "minor": {"acc": 0.47863247863247865}}, "medical_genetics_test": {"major": 0.22, "minor": {"acc": 0.22}}, "miscellaneous_test": {"major": 0.284, "minor": {"acc": 0.284}}, "moral_disputes_test": {"major": 0.2630057803468208, "minor": {"acc": 0.2630057803468208}}, "moral_scenarios_test": {"major": 0.252, "minor": {"acc": 0.252}}, "nutrition_test": {"major": 0.24509803921568626, "minor": {"acc": 0.24509803921568626}}, "philosophy_test": {"major": 0.2604501607717042, "minor": {"acc": 0.2604501607717042}}, "prehistory_test": {"major": 0.23765432098765432, "minor": {"acc": 0.23765432098765432}}, "professional_accounting_test": {"major": 0.25886524822695034, "minor": {"acc": 0.25886524822695034}}, "professional_law_test": {"major": 0.242, "minor": {"acc": 0.242}}, "professional_medicine_test": {"major": 0.29411764705882354, "minor": {"acc": 0.29411764705882354}}, "professional_psychology_test": {"major": 0.244, "minor": {"acc": 0.244}}, "public_relations_test": {"major": 0.23636363636363636, "minor": {"acc": 0.23636363636363636}}, "security_studies_test": {"major": 0.22040816326530613, "minor": {"acc": 0.22040816326530613}}, "sociology_test": {"major": 0.2935323383084577, "minor": {"acc": 0.2935323383084577}}, "us_foreign_policy_test": {"major": 0.24, "minor": {"acc": 0.24}}, "virology_test": {"major": 0.21084337349397592, "minor": {"acc": 0.21084337349397592}}, "world_religions_test": {"major": 0.24561403508771928, "minor": {"acc": 0.24561403508771928}}}, "train_state": {"global_steps": 171, "task_steps": {"abstract_algebra_test": 3, "anatomy_test": 2, "astronomy_test": 3, "business_ethics_test": 3, "clinical_knowledge_test": 3, "college_biology_test": 3, "college_chemistry_test": 3, "college_computer_science_test": 3, "college_mathematics_test": 2, "college_medicine_test": 3, "college_physics_test": 2, "computer_security_test": 2, "conceptual_physics_test": 2, "econometrics_test": 4, "electrical_engineering_test": 3, "elementary_mathematics_test": 3, "formal_logic_test": 3, "global_facts_test": 2, "high_school_biology_test": 3, "high_school_chemistry_test": 3, "high_school_computer_science_test": 3, "high_school_european_history_test": 3, "high_school_geography_test": 3, "high_school_government_and_politics_test": 3, "high_school_macroeconomics_test": 3, "high_school_mathematics_test": 3, "high_school_microeconomics_test": 4, "high_school_physics_test": 3, "high_school_psychology_test": 3, "high_school_statistics_test": 3, "high_school_us_history_test": 3, "high_school_world_history_test": 2, "human_aging_test": 3, "human_sexuality_test": 3, "international_law_test": 4, "jurisprudence_test": 2, "logical_fallacies_test": 3, "machine_learning_test": 4, "management_test": 4, "marketing_test": 3, "medical_genetics_test": 3, "miscellaneous_test": 4, "moral_disputes_test": 3, "moral_scenarios_test": 3, "nutrition_test": 3, "philosophy_test": 3, "prehistory_test": 3, "professional_accounting_test": 4, "professional_law_test": 2, "professional_medicine_test": 3, "professional_psychology_test": 5, "public_relations_test": 3, "security_studies_test": 3, "sociology_test": 3, "us_foreign_policy_test": 3, "virology_test": 2, "world_religions_test": 4}}, "TIMESTAMP": 1638761190.9108775}
{"score": 0.25858020745608984, "metrics": {"abstract_algebra_test": {"major": 0.19, "minor": {"acc": 0.19}}, "anatomy_test": {"major": 0.21481481481481482, "minor": {"acc": 0.21481481481481482}}, "astronomy_test": {"major": 0.28289473684210525, "minor": {"acc": 0.28289473684210525}}, "business_ethics_test": {"major": 0.36, "minor": {"acc": 0.36}}, "clinical_knowledge_test": {"major": 0.27169811320754716, "minor": {"acc": 0.27169811320754716}}, "college_biology_test": {"major": 0.2222222222222222, "minor": {"acc": 0.2222222222222222}}, "college_chemistry_test": {"major": 0.27, "minor": {"acc": 0.27}}, "college_computer_science_test": {"major": 0.25, "minor": {"acc": 0.25}}, "college_mathematics_test": {"major": 0.18, "minor": {"acc": 0.18}}, "college_medicine_test": {"major": 0.21965317919075145, "minor": {"acc": 0.21965317919075145}}, "college_physics_test": {"major": 0.30392156862745096, "minor": {"acc": 0.30392156862745096}}, "computer_security_test": {"major": 0.26, "minor": {"acc": 0.26}}, "conceptual_physics_test": {"major": 0.2978723404255319, "minor": {"acc": 0.2978723404255319}}, "econometrics_test": {"major": 0.2543859649122807, "minor": {"acc": 0.2543859649122807}}, "electrical_engineering_test": {"major": 0.2413793103448276, "minor": {"acc": 0.2413793103448276}}, "elementary_mathematics_test": {"major": 0.2037037037037037, "minor": {"acc": 0.2037037037037037}}, "formal_logic_test": {"major": 0.2619047619047619, "minor": {"acc": 0.2619047619047619}}, "global_facts_test": {"major": 0.3, "minor": {"acc": 0.3}}, "high_school_biology_test": {"major": 0.24193548387096775, "minor": {"acc": 0.24193548387096775}}, "high_school_chemistry_test": {"major": 0.22660098522167488, "minor": {"acc": 0.22660098522167488}}, "high_school_computer_science_test": {"major": 0.21, "minor": {"acc": 0.21}}, "high_school_european_history_test": {"major": 0.16363636363636364, "minor": {"acc": 0.16363636363636364}}, "high_school_geography_test": {"major": 0.37373737373737376, "minor": {"acc": 0.37373737373737376}}, "high_school_government_and_politics_test": {"major": 0.32124352331606215, "minor": {"acc": 0.32124352331606215}}, "high_school_macroeconomics_test": {"major": 0.2076923076923077, "minor": {"acc": 0.2076923076923077}}, "high_school_mathematics_test": {"major": 0.1925925925925926, "minor": {"acc": 0.1925925925925926}}, "high_school_microeconomics_test": {"major": 0.2647058823529412, "minor": {"acc": 0.2647058823529412}}, "high_school_physics_test": {"major": 0.2251655629139073, "minor": {"acc": 0.2251655629139073}}, "high_school_psychology_test": {"major": 0.25, "minor": {"acc": 0.25}}, "high_school_statistics_test": {"major": 0.24074074074074073, "minor": {"acc": 0.24074074074074073}}, "high_school_us_history_test": {"major": 0.3431372549019608, "minor": {"acc": 0.3431372549019608}}, "high_school_world_history_test": {"major": 0.189873417721519, "minor": {"acc": 0.189873417721519}}, "human_aging_test": {"major": 0.2645739910313901, "minor": {"acc": 0.2645739910313901}}, "human_sexuality_test": {"major": 0.2366412213740458, "minor": {"acc": 0.2366412213740458}}, "international_law_test": {"major": 0.3140495867768595, "minor": {"acc": 0.3140495867768595}}, "jurisprudence_test": {"major": 0.2222222222222222, "minor": {"acc": 0.2222222222222222}}, "logical_fallacies_test": {"major": 0.3803680981595092, "minor": {"acc": 0.3803680981595092}}, "machine_learning_test": {"major": 0.26785714285714285, "minor": {"acc": 0.26785714285714285}}, "management_test": {"major": 0.2912621359223301, "minor": {"acc": 0.2912621359223301}}, "marketing_test": {"major": 0.47863247863247865, "minor": {"acc": 0.47863247863247865}}, "medical_genetics_test": {"major": 0.22, "minor": {"acc": 0.22}}, "miscellaneous_test": {"major": 0.284, "minor": {"acc": 0.284}}, "moral_disputes_test": {"major": 0.2630057803468208, "minor": {"acc": 0.2630057803468208}}, "moral_scenarios_test": {"major": 0.252, "minor": {"acc": 0.252}}, "nutrition_test": {"major": 0.24509803921568626, "minor": {"acc": 0.24509803921568626}}, "philosophy_test": {"major": 0.2604501607717042, "minor": {"acc": 0.2604501607717042}}, "prehistory_test": {"major": 0.23765432098765432, "minor": {"acc": 0.23765432098765432}}, "professional_accounting_test": {"major": 0.25886524822695034, "minor": {"acc": 0.25886524822695034}}, "professional_law_test": {"major": 0.242, "minor": {"acc": 0.242}}, "professional_medicine_test": {"major": 0.29411764705882354, "minor": {"acc": 0.29411764705882354}}, "professional_psychology_test": {"major": 0.244, "minor": {"acc": 0.244}}, "public_relations_test": {"major": 0.23636363636363636, "minor": {"acc": 0.23636363636363636}}, "security_studies_test": {"major": 0.22040816326530613, "minor": {"acc": 0.22040816326530613}}, "sociology_test": {"major": 0.2935323383084577, "minor": {"acc": 0.2935323383084577}}, "us_foreign_policy_test": {"major": 0.24, "minor": {"acc": 0.24}}, "virology_test": {"major": 0.21084337349397592, "minor": {"acc": 0.21084337349397592}}, "world_religions_test": {"major": 0.24561403508771928, "minor": {"acc": 0.24561403508771928}}}, "train_state": {"global_steps": 171, "task_steps": {"abstract_algebra_test": 3, "anatomy_test": 2, "astronomy_test": 3, "business_ethics_test": 3, "clinical_knowledge_test": 3, "college_biology_test": 3, "college_chemistry_test": 3, "college_computer_science_test": 3, "college_mathematics_test": 2, "college_medicine_test": 3, "college_physics_test": 2, "computer_security_test": 2, "conceptual_physics_test": 2, "econometrics_test": 4, "electrical_engineering_test": 3, "elementary_mathematics_test": 3, "formal_logic_test": 3, "global_facts_test": 2, "high_school_biology_test": 3, "high_school_chemistry_test": 3, "high_school_computer_science_test": 3, "high_school_european_history_test": 3, "high_school_geography_test": 3, "high_school_government_and_politics_test": 3, "high_school_macroeconomics_test": 3, "high_school_mathematics_test": 3, "high_school_microeconomics_test": 4, "high_school_physics_test": 3, "high_school_psychology_test": 3, "high_school_statistics_test": 3, "high_school_us_history_test": 3, "high_school_world_history_test": 2, "human_aging_test": 3, "human_sexuality_test": 3, "international_law_test": 4, "jurisprudence_test": 2, "logical_fallacies_test": 3, "machine_learning_test": 4, "management_test": 4, "marketing_test": 3, "medical_genetics_test": 3, "miscellaneous_test": 4, "moral_disputes_test": 3, "moral_scenarios_test": 3, "nutrition_test": 3, "philosophy_test": 3, "prehistory_test": 3, "professional_accounting_test": 4, "professional_law_test": 2, "professional_medicine_test": 3, "professional_psychology_test": 5, "public_relations_test": 3, "security_studies_test": 3, "sociology_test": 3, "us_foreign_policy_test": 3, "virology_test": 2, "world_religions_test": 4}}, "TIMESTAMP": 1638761236.5398962}

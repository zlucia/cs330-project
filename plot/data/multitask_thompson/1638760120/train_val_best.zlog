{"score": 0.2563112605373544, "metrics": {"abstract_algebra_test": {"major": 0.26, "minor": {"acc": 0.26}}, "anatomy_test": {"major": 0.23703703703703705, "minor": {"acc": 0.23703703703703705}}, "astronomy_test": {"major": 0.26973684210526316, "minor": {"acc": 0.26973684210526316}}, "business_ethics_test": {"major": 0.25, "minor": {"acc": 0.25}}, "clinical_knowledge_test": {"major": 0.24150943396226415, "minor": {"acc": 0.24150943396226415}}, "college_biology_test": {"major": 0.2361111111111111, "minor": {"acc": 0.2361111111111111}}, "college_chemistry_test": {"major": 0.15, "minor": {"acc": 0.15}}, "college_computer_science_test": {"major": 0.24, "minor": {"acc": 0.24}}, "college_mathematics_test": {"major": 0.25, "minor": {"acc": 0.25}}, "college_medicine_test": {"major": 0.2254335260115607, "minor": {"acc": 0.2254335260115607}}, "college_physics_test": {"major": 0.3137254901960784, "minor": {"acc": 0.3137254901960784}}, "computer_security_test": {"major": 0.19, "minor": {"acc": 0.19}}, "conceptual_physics_test": {"major": 0.28085106382978725, "minor": {"acc": 0.28085106382978725}}, "econometrics_test": {"major": 0.2982456140350877, "minor": {"acc": 0.2982456140350877}}, "electrical_engineering_test": {"major": 0.31724137931034485, "minor": {"acc": 0.31724137931034485}}, "elementary_mathematics_test": {"major": 0.23015873015873015, "minor": {"acc": 0.23015873015873015}}, "formal_logic_test": {"major": 0.3333333333333333, "minor": {"acc": 0.3333333333333333}}, "global_facts_test": {"major": 0.21, "minor": {"acc": 0.21}}, "high_school_biology_test": {"major": 0.27419354838709675, "minor": {"acc": 0.27419354838709675}}, "high_school_chemistry_test": {"major": 0.27586206896551724, "minor": {"acc": 0.27586206896551724}}, "high_school_computer_science_test": {"major": 0.25, "minor": {"acc": 0.25}}, "high_school_european_history_test": {"major": 0.2787878787878788, "minor": {"acc": 0.2787878787878788}}, "high_school_geography_test": {"major": 0.17676767676767677, "minor": {"acc": 0.17676767676767677}}, "high_school_government_and_politics_test": {"major": 0.31088082901554404, "minor": {"acc": 0.31088082901554404}}, "high_school_macroeconomics_test": {"major": 0.19230769230769232, "minor": {"acc": 0.19230769230769232}}, "high_school_mathematics_test": {"major": 0.2222222222222222, "minor": {"acc": 0.2222222222222222}}, "high_school_microeconomics_test": {"major": 0.22268907563025211, "minor": {"acc": 0.22268907563025211}}, "high_school_physics_test": {"major": 0.2847682119205298, "minor": {"acc": 0.2847682119205298}}, "high_school_psychology_test": {"major": 0.28, "minor": {"acc": 0.28}}, "high_school_statistics_test": {"major": 0.2824074074074074, "minor": {"acc": 0.2824074074074074}}, "high_school_us_history_test": {"major": 0.27450980392156865, "minor": {"acc": 0.27450980392156865}}, "high_school_world_history_test": {"major": 0.15611814345991562, "minor": {"acc": 0.15611814345991562}}, "human_aging_test": {"major": 0.23318385650224216, "minor": {"acc": 0.23318385650224216}}, "human_sexuality_test": {"major": 0.2748091603053435, "minor": {"acc": 0.2748091603053435}}, "international_law_test": {"major": 0.34710743801652894, "minor": {"acc": 0.34710743801652894}}, "jurisprudence_test": {"major": 0.23148148148148148, "minor": {"acc": 0.23148148148148148}}, "logical_fallacies_test": {"major": 0.294478527607362, "minor": {"acc": 0.294478527607362}}, "machine_learning_test": {"major": 0.25, "minor": {"acc": 0.25}}, "management_test": {"major": 0.30097087378640774, "minor": {"acc": 0.30097087378640774}}, "marketing_test": {"major": 0.2863247863247863, "minor": {"acc": 0.2863247863247863}}, "medical_genetics_test": {"major": 0.23, "minor": {"acc": 0.23}}, "miscellaneous_test": {"major": 0.202, "minor": {"acc": 0.202}}, "moral_disputes_test": {"major": 0.22832369942196531, "minor": {"acc": 0.22832369942196531}}, "moral_scenarios_test": {"major": 0.234, "minor": {"acc": 0.234}}, "nutrition_test": {"major": 0.23202614379084968, "minor": {"acc": 0.23202614379084968}}, "philosophy_test": {"major": 0.24437299035369775, "minor": {"acc": 0.24437299035369775}}, "prehistory_test": {"major": 0.24691358024691357, "minor": {"acc": 0.24691358024691357}}, "professional_accounting_test": {"major": 0.25177304964539005, "minor": {"acc": 0.25177304964539005}}, "professional_law_test": {"major": 0.298, "minor": {"acc": 0.298}}, "professional_medicine_test": {"major": 0.25735294117647056, "minor": {"acc": 0.25735294117647056}}, "professional_psychology_test": {"major": 0.25, "minor": {"acc": 0.25}}, "public_relations_test": {"major": 0.22727272727272727, "minor": {"acc": 0.22727272727272727}}, "security_studies_test": {"major": 0.2653061224489796, "minor": {"acc": 0.2653061224489796}}, "sociology_test": {"major": 0.31840796019900497, "minor": {"acc": 0.31840796019900497}}, "us_foreign_policy_test": {"major": 0.25, "minor": {"acc": 0.25}}, "virology_test": {"major": 0.3132530120481928, "minor": {"acc": 0.3132530120481928}}, "world_religions_test": {"major": 0.32748538011695905, "minor": {"acc": 0.32748538011695905}}}, "train_state": {"global_steps": 29, "task_steps": {"abstract_algebra_test": 2, "anatomy_test": 1, "astronomy_test": 1, "business_ethics_test": 0, "clinical_knowledge_test": 0, "college_biology_test": 1, "college_chemistry_test": 1, "college_computer_science_test": 0, "college_mathematics_test": 1, "college_medicine_test": 0, "college_physics_test": 1, "computer_security_test": 0, "conceptual_physics_test": 0, "econometrics_test": 0, "electrical_engineering_test": 1, "elementary_mathematics_test": 0, "formal_logic_test": 1, "global_facts_test": 0, "high_school_biology_test": 1, "high_school_chemistry_test": 0, "high_school_computer_science_test": 1, "high_school_european_history_test": 2, "high_school_geography_test": 0, "high_school_government_and_politics_test": 2, "high_school_macroeconomics_test": 1, "high_school_mathematics_test": 1, "high_school_microeconomics_test": 0, "high_school_physics_test": 0, "high_school_psychology_test": 0, "high_school_statistics_test": 0, "high_school_us_history_test": 0, "high_school_world_history_test": 0, "human_aging_test": 0, "human_sexuality_test": 2, "international_law_test": 1, "jurisprudence_test": 0, "logical_fallacies_test": 0, "machine_learning_test": 0, "management_test": 0, "marketing_test": 0, "medical_genetics_test": 1, "miscellaneous_test": 0, "moral_disputes_test": 0, "moral_scenarios_test": 0, "nutrition_test": 0, "philosophy_test": 0, "prehistory_test": 0, "professional_accounting_test": 1, "professional_law_test": 1, "professional_medicine_test": 0, "professional_psychology_test": 0, "public_relations_test": 0, "security_studies_test": 1, "sociology_test": 1, "us_foreign_policy_test": 1, "virology_test": 0, "world_religions_test": 2}}, "TIMESTAMP": 1638760312.2882495}
{"score": 0.25907039997868336, "metrics": {"abstract_algebra_test": {"major": 0.23, "minor": {"acc": 0.23}}, "anatomy_test": {"major": 0.28888888888888886, "minor": {"acc": 0.28888888888888886}}, "astronomy_test": {"major": 0.3092105263157895, "minor": {"acc": 0.3092105263157895}}, "business_ethics_test": {"major": 0.34, "minor": {"acc": 0.34}}, "clinical_knowledge_test": {"major": 0.2641509433962264, "minor": {"acc": 0.2641509433962264}}, "college_biology_test": {"major": 0.24305555555555555, "minor": {"acc": 0.24305555555555555}}, "college_chemistry_test": {"major": 0.2, "minor": {"acc": 0.2}}, "college_computer_science_test": {"major": 0.17, "minor": {"acc": 0.17}}, "college_mathematics_test": {"major": 0.32, "minor": {"acc": 0.32}}, "college_medicine_test": {"major": 0.23121387283236994, "minor": {"acc": 0.23121387283236994}}, "college_physics_test": {"major": 0.27450980392156865, "minor": {"acc": 0.27450980392156865}}, "computer_security_test": {"major": 0.21, "minor": {"acc": 0.21}}, "conceptual_physics_test": {"major": 0.28085106382978725, "minor": {"acc": 0.28085106382978725}}, "econometrics_test": {"major": 0.2982456140350877, "minor": {"acc": 0.2982456140350877}}, "electrical_engineering_test": {"major": 0.25517241379310346, "minor": {"acc": 0.25517241379310346}}, "elementary_mathematics_test": {"major": 0.22486772486772486, "minor": {"acc": 0.22486772486772486}}, "formal_logic_test": {"major": 0.2857142857142857, "minor": {"acc": 0.2857142857142857}}, "global_facts_test": {"major": 0.28, "minor": {"acc": 0.28}}, "high_school_biology_test": {"major": 0.25483870967741934, "minor": {"acc": 0.25483870967741934}}, "high_school_chemistry_test": {"major": 0.26108374384236455, "minor": {"acc": 0.26108374384236455}}, "high_school_computer_science_test": {"major": 0.21, "minor": {"acc": 0.21}}, "high_school_european_history_test": {"major": 0.16363636363636364, "minor": {"acc": 0.16363636363636364}}, "high_school_geography_test": {"major": 0.23232323232323232, "minor": {"acc": 0.23232323232323232}}, "high_school_government_and_politics_test": {"major": 0.31088082901554404, "minor": {"acc": 0.31088082901554404}}, "high_school_macroeconomics_test": {"major": 0.21025641025641026, "minor": {"acc": 0.21025641025641026}}, "high_school_mathematics_test": {"major": 0.2037037037037037, "minor": {"acc": 0.2037037037037037}}, "high_school_microeconomics_test": {"major": 0.25630252100840334, "minor": {"acc": 0.25630252100840334}}, "high_school_physics_test": {"major": 0.25165562913907286, "minor": {"acc": 0.25165562913907286}}, "high_school_psychology_test": {"major": 0.28, "minor": {"acc": 0.28}}, "high_school_statistics_test": {"major": 0.3148148148148148, "minor": {"acc": 0.3148148148148148}}, "high_school_us_history_test": {"major": 0.25980392156862747, "minor": {"acc": 0.25980392156862747}}, "high_school_world_history_test": {"major": 0.1518987341772152, "minor": {"acc": 0.1518987341772152}}, "human_aging_test": {"major": 0.23318385650224216, "minor": {"acc": 0.23318385650224216}}, "human_sexuality_test": {"major": 0.2824427480916031, "minor": {"acc": 0.2824427480916031}}, "international_law_test": {"major": 0.3140495867768595, "minor": {"acc": 0.3140495867768595}}, "jurisprudence_test": {"major": 0.17592592592592593, "minor": {"acc": 0.17592592592592593}}, "logical_fallacies_test": {"major": 0.31901840490797545, "minor": {"acc": 0.31901840490797545}}, "machine_learning_test": {"major": 0.2767857142857143, "minor": {"acc": 0.2767857142857143}}, "management_test": {"major": 0.2621359223300971, "minor": {"acc": 0.2621359223300971}}, "marketing_test": {"major": 0.45726495726495725, "minor": {"acc": 0.45726495726495725}}, "medical_genetics_test": {"major": 0.26, "minor": {"acc": 0.26}}, "miscellaneous_test": {"major": 0.308, "minor": {"acc": 0.308}}, "moral_disputes_test": {"major": 0.24566473988439305, "minor": {"acc": 0.24566473988439305}}, "moral_scenarios_test": {"major": 0.252, "minor": {"acc": 0.252}}, "nutrition_test": {"major": 0.2679738562091503, "minor": {"acc": 0.2679738562091503}}, "philosophy_test": {"major": 0.2508038585209003, "minor": {"acc": 0.2508038585209003}}, "prehistory_test": {"major": 0.2191358024691358, "minor": {"acc": 0.2191358024691358}}, "professional_accounting_test": {"major": 0.2375886524822695, "minor": {"acc": 0.2375886524822695}}, "professional_law_test": {"major": 0.29, "minor": {"acc": 0.29}}, "professional_medicine_test": {"major": 0.2867647058823529, "minor": {"acc": 0.2867647058823529}}, "professional_psychology_test": {"major": 0.24, "minor": {"acc": 0.24}}, "public_relations_test": {"major": 0.23636363636363636, "minor": {"acc": 0.23636363636363636}}, "security_studies_test": {"major": 0.22448979591836735, "minor": {"acc": 0.22448979591836735}}, "sociology_test": {"major": 0.2935323383084577, "minor": {"acc": 0.2935323383084577}}, "us_foreign_policy_test": {"major": 0.24, "minor": {"acc": 0.24}}, "virology_test": {"major": 0.21686746987951808, "minor": {"acc": 0.21686746987951808}}, "world_religions_test": {"major": 0.30994152046783624, "minor": {"acc": 0.30994152046783624}}}, "train_state": {"global_steps": 59, "task_steps": {"abstract_algebra_test": 2, "anatomy_test": 2, "astronomy_test": 1, "business_ethics_test": 1, "clinical_knowledge_test": 2, "college_biology_test": 2, "college_chemistry_test": 2, "college_computer_science_test": 1, "college_mathematics_test": 1, "college_medicine_test": 0, "college_physics_test": 1, "computer_security_test": 0, "conceptual_physics_test": 0, "econometrics_test": 0, "electrical_engineering_test": 1, "elementary_mathematics_test": 1, "formal_logic_test": 2, "global_facts_test": 0, "high_school_biology_test": 1, "high_school_chemistry_test": 1, "high_school_computer_science_test": 1, "high_school_european_history_test": 2, "high_school_geography_test": 0, "high_school_government_and_politics_test": 3, "high_school_macroeconomics_test": 2, "high_school_mathematics_test": 1, "high_school_microeconomics_test": 1, "high_school_physics_test": 0, "high_school_psychology_test": 0, "high_school_statistics_test": 0, "high_school_us_history_test": 1, "high_school_world_history_test": 2, "human_aging_test": 0, "human_sexuality_test": 3, "international_law_test": 1, "jurisprudence_test": 1, "logical_fallacies_test": 0, "machine_learning_test": 2, "management_test": 1, "marketing_test": 1, "medical_genetics_test": 1, "miscellaneous_test": 1, "moral_disputes_test": 0, "moral_scenarios_test": 1, "nutrition_test": 0, "philosophy_test": 1, "prehistory_test": 0, "professional_accounting_test": 1, "professional_law_test": 2, "professional_medicine_test": 1, "professional_psychology_test": 0, "public_relations_test": 0, "security_studies_test": 1, "sociology_test": 2, "us_foreign_policy_test": 3, "virology_test": 0, "world_religions_test": 2}}, "TIMESTAMP": 1638760532.5465193}
{"score": 0.2611657019894215, "metrics": {"abstract_algebra_test": {"major": 0.29, "minor": {"acc": 0.29}}, "anatomy_test": {"major": 0.22962962962962963, "minor": {"acc": 0.22962962962962963}}, "astronomy_test": {"major": 0.2894736842105263, "minor": {"acc": 0.2894736842105263}}, "business_ethics_test": {"major": 0.38, "minor": {"acc": 0.38}}, "clinical_knowledge_test": {"major": 0.2679245283018868, "minor": {"acc": 0.2679245283018868}}, "college_biology_test": {"major": 0.24305555555555555, "minor": {"acc": 0.24305555555555555}}, "college_chemistry_test": {"major": 0.22, "minor": {"acc": 0.22}}, "college_computer_science_test": {"major": 0.24, "minor": {"acc": 0.24}}, "college_mathematics_test": {"major": 0.24, "minor": {"acc": 0.24}}, "college_medicine_test": {"major": 0.21965317919075145, "minor": {"acc": 0.21965317919075145}}, "college_physics_test": {"major": 0.3333333333333333, "minor": {"acc": 0.3333333333333333}}, "computer_security_test": {"major": 0.32, "minor": {"acc": 0.32}}, "conceptual_physics_test": {"major": 0.2978723404255319, "minor": {"acc": 0.2978723404255319}}, "econometrics_test": {"major": 0.2894736842105263, "minor": {"acc": 0.2894736842105263}}, "electrical_engineering_test": {"major": 0.2620689655172414, "minor": {"acc": 0.2620689655172414}}, "elementary_mathematics_test": {"major": 0.21164021164021163, "minor": {"acc": 0.21164021164021163}}, "formal_logic_test": {"major": 0.2857142857142857, "minor": {"acc": 0.2857142857142857}}, "global_facts_test": {"major": 0.22, "minor": {"acc": 0.22}}, "high_school_biology_test": {"major": 0.25161290322580643, "minor": {"acc": 0.25161290322580643}}, "high_school_chemistry_test": {"major": 0.23645320197044334, "minor": {"acc": 0.23645320197044334}}, "high_school_computer_science_test": {"major": 0.22, "minor": {"acc": 0.22}}, "high_school_european_history_test": {"major": 0.16363636363636364, "minor": {"acc": 0.16363636363636364}}, "high_school_geography_test": {"major": 0.3686868686868687, "minor": {"acc": 0.3686868686868687}}, "high_school_government_and_politics_test": {"major": 0.29015544041450775, "minor": {"acc": 0.29015544041450775}}, "high_school_macroeconomics_test": {"major": 0.2153846153846154, "minor": {"acc": 0.2153846153846154}}, "high_school_mathematics_test": {"major": 0.2074074074074074, "minor": {"acc": 0.2074074074074074}}, "high_school_microeconomics_test": {"major": 0.23109243697478993, "minor": {"acc": 0.23109243697478993}}, "high_school_physics_test": {"major": 0.2251655629139073, "minor": {"acc": 0.2251655629139073}}, "high_school_psychology_test": {"major": 0.28, "minor": {"acc": 0.28}}, "high_school_statistics_test": {"major": 0.2638888888888889, "minor": {"acc": 0.2638888888888889}}, "high_school_us_history_test": {"major": 0.29901960784313725, "minor": {"acc": 0.29901960784313725}}, "high_school_world_history_test": {"major": 0.189873417721519, "minor": {"acc": 0.189873417721519}}, "human_aging_test": {"major": 0.28699551569506726, "minor": {"acc": 0.28699551569506726}}, "human_sexuality_test": {"major": 0.2366412213740458, "minor": {"acc": 0.2366412213740458}}, "international_law_test": {"major": 0.32231404958677684, "minor": {"acc": 0.32231404958677684}}, "jurisprudence_test": {"major": 0.2037037037037037, "minor": {"acc": 0.2037037037037037}}, "logical_fallacies_test": {"major": 0.3619631901840491, "minor": {"acc": 0.3619631901840491}}, "machine_learning_test": {"major": 0.26785714285714285, "minor": {"acc": 0.26785714285714285}}, "management_test": {"major": 0.27184466019417475, "minor": {"acc": 0.27184466019417475}}, "marketing_test": {"major": 0.49572649572649574, "minor": {"acc": 0.49572649572649574}}, "medical_genetics_test": {"major": 0.22, "minor": {"acc": 0.22}}, "miscellaneous_test": {"major": 0.306, "minor": {"acc": 0.306}}, "moral_disputes_test": {"major": 0.24566473988439305, "minor": {"acc": 0.24566473988439305}}, "moral_scenarios_test": {"major": 0.256, "minor": {"acc": 0.256}}, "nutrition_test": {"major": 0.2222222222222222, "minor": {"acc": 0.2222222222222222}}, "philosophy_test": {"major": 0.24115755627009647, "minor": {"acc": 0.24115755627009647}}, "prehistory_test": {"major": 0.25, "minor": {"acc": 0.25}}, "professional_accounting_test": {"major": 0.26595744680851063, "minor": {"acc": 0.26595744680851063}}, "professional_law_test": {"major": 0.262, "minor": {"acc": 0.262}}, "professional_medicine_test": {"major": 0.28308823529411764, "minor": {"acc": 0.28308823529411764}}, "professional_psychology_test": {"major": 0.242, "minor": {"acc": 0.242}}, "public_relations_test": {"major": 0.2, "minor": {"acc": 0.2}}, "security_studies_test": {"major": 0.2163265306122449, "minor": {"acc": 0.2163265306122449}}, "sociology_test": {"major": 0.2885572139303483, "minor": {"acc": 0.2885572139303483}}, "us_foreign_policy_test": {"major": 0.22, "minor": {"acc": 0.22}}, "virology_test": {"major": 0.18674698795180722, "minor": {"acc": 0.18674698795180722}}, "world_religions_test": {"major": 0.25146198830409355, "minor": {"acc": 0.25146198830409355}}}, "train_state": {"global_steps": 89, "task_steps": {"abstract_algebra_test": 2, "anatomy_test": 2, "astronomy_test": 1, "business_ethics_test": 1, "clinical_knowledge_test": 2, "college_biology_test": 2, "college_chemistry_test": 2, "college_computer_science_test": 2, "college_mathematics_test": 1, "college_medicine_test": 1, "college_physics_test": 1, "computer_security_test": 1, "conceptual_physics_test": 1, "econometrics_test": 2, "electrical_engineering_test": 1, "elementary_mathematics_test": 2, "formal_logic_test": 2, "global_facts_test": 0, "high_school_biology_test": 2, "high_school_chemistry_test": 1, "high_school_computer_science_test": 3, "high_school_european_history_test": 2, "high_school_geography_test": 1, "high_school_government_and_politics_test": 3, "high_school_macroeconomics_test": 2, "high_school_mathematics_test": 2, "high_school_microeconomics_test": 2, "high_school_physics_test": 1, "high_school_psychology_test": 0, "high_school_statistics_test": 2, "high_school_us_history_test": 1, "high_school_world_history_test": 2, "human_aging_test": 2, "human_sexuality_test": 3, "international_law_test": 1, "jurisprudence_test": 1, "logical_fallacies_test": 1, "machine_learning_test": 2, "management_test": 2, "marketing_test": 2, "medical_genetics_test": 1, "miscellaneous_test": 1, "moral_disputes_test": 2, "moral_scenarios_test": 2, "nutrition_test": 2, "philosophy_test": 1, "prehistory_test": 2, "professional_accounting_test": 2, "professional_law_test": 2, "professional_medicine_test": 1, "professional_psychology_test": 0, "public_relations_test": 0, "security_studies_test": 1, "sociology_test": 2, "us_foreign_policy_test": 3, "virology_test": 0, "world_religions_test": 3}}, "TIMESTAMP": 1638760695.4456222}

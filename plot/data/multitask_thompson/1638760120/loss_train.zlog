{"task": "world_religions_test", "task_step": 1, "global_step": 1, "loss_val": 1.475804090499878, "TIMESTAMP": 1638760134.131857}
{"task": "high_school_european_history_test", "task_step": 1, "global_step": 2, "loss_val": 1.3887096643447876, "TIMESTAMP": 1638760138.717425}
{"task": "us_foreign_policy_test", "task_step": 1, "global_step": 3, "loss_val": 1.4288921356201172, "TIMESTAMP": 1638760143.2589445}
{"task": "college_mathematics_test", "task_step": 1, "global_step": 4, "loss_val": 1.4256436824798584, "TIMESTAMP": 1638760147.952588}
{"task": "human_sexuality_test", "task_step": 1, "global_step": 5, "loss_val": 1.449761152267456, "TIMESTAMP": 1638760152.5298493}
{"task": "college_chemistry_test", "task_step": 1, "global_step": 6, "loss_val": 1.3851850032806396, "TIMESTAMP": 1638760157.0952673}
{"task": "sociology_test", "task_step": 1, "global_step": 7, "loss_val": 1.3913427591323853, "TIMESTAMP": 1638760161.6646893}
{"task": "high_school_macroeconomics_test", "task_step": 1, "global_step": 8, "loss_val": 1.4157688617706299, "TIMESTAMP": 1638760166.1876268}
{"task": "medical_genetics_test", "task_step": 1, "global_step": 9, "loss_val": 1.490661859512329, "TIMESTAMP": 1638760170.7080603}
{"task": "high_school_government_and_politics_test", "task_step": 1, "global_step": 10, "loss_val": 1.4610068798065186, "TIMESTAMP": 1638760175.2324018}
{"task": "abstract_algebra_test", "task_step": 1, "global_step": 11, "loss_val": 1.3846641778945923, "TIMESTAMP": 1638760179.747725}
{"task": "college_physics_test", "task_step": 1, "global_step": 12, "loss_val": 1.4066909551620483, "TIMESTAMP": 1638760184.288503}
{"task": "professional_law_test", "task_step": 1, "global_step": 13, "loss_val": 1.3218022584915161, "TIMESTAMP": 1638760188.8166146}
{"task": "astronomy_test", "task_step": 1, "global_step": 14, "loss_val": 1.3688331842422485, "TIMESTAMP": 1638760193.3494267}
{"task": "high_school_government_and_politics_test", "task_step": 2, "global_step": 15, "loss_val": 1.4558868408203125, "TIMESTAMP": 1638760197.9730108}
{"task": "professional_accounting_test", "task_step": 1, "global_step": 16, "loss_val": 1.3735767602920532, "TIMESTAMP": 1638760202.5027132}
{"task": "formal_logic_test", "task_step": 1, "global_step": 17, "loss_val": 1.3304029703140259, "TIMESTAMP": 1638760207.0254126}
{"task": "anatomy_test", "task_step": 1, "global_step": 18, "loss_val": 1.3917509317398071, "TIMESTAMP": 1638760211.5648713}
{"task": "international_law_test", "task_step": 1, "global_step": 19, "loss_val": 1.3576250076293945, "TIMESTAMP": 1638760216.1214175}
{"task": "high_school_biology_test", "task_step": 1, "global_step": 20, "loss_val": 1.377290964126587, "TIMESTAMP": 1638760220.671529}
{"task": "electrical_engineering_test", "task_step": 1, "global_step": 21, "loss_val": 1.414337396621704, "TIMESTAMP": 1638760225.2085583}
{"task": "college_biology_test", "task_step": 1, "global_step": 22, "loss_val": 1.3826098442077637, "TIMESTAMP": 1638760229.753514}
{"task": "high_school_mathematics_test", "task_step": 1, "global_step": 23, "loss_val": 1.3839927911758423, "TIMESTAMP": 1638760234.3008258}
{"task": "human_sexuality_test", "task_step": 2, "global_step": 24, "loss_val": 1.4136244058609009, "TIMESTAMP": 1638760238.7930446}
{"task": "world_religions_test", "task_step": 2, "global_step": 25, "loss_val": 1.3901029825210571, "TIMESTAMP": 1638760243.311241}
{"task": "security_studies_test", "task_step": 1, "global_step": 26, "loss_val": 1.3410800695419312, "TIMESTAMP": 1638760247.9424846}
{"task": "abstract_algebra_test", "task_step": 2, "global_step": 27, "loss_val": 1.3935781717300415, "TIMESTAMP": 1638760252.4489858}
{"task": "high_school_computer_science_test", "task_step": 1, "global_step": 28, "loss_val": 1.4728041887283325, "TIMESTAMP": 1638760256.9797804}
{"task": "high_school_european_history_test", "task_step": 2, "global_step": 29, "loss_val": 1.4163872003555298, "TIMESTAMP": 1638760261.493817}
{"task": "elementary_mathematics_test", "task_step": 1, "global_step": 30, "loss_val": 1.4244704246520996, "TIMESTAMP": 1638760313.118267}
{"task": "college_chemistry_test", "task_step": 2, "global_step": 31, "loss_val": 1.3819996118545532, "TIMESTAMP": 1638760318.8793983}
{"task": "us_foreign_policy_test", "task_step": 2, "global_step": 32, "loss_val": 1.378989815711975, "TIMESTAMP": 1638760324.6629314}
{"task": "clinical_knowledge_test", "task_step": 1, "global_step": 33, "loss_val": 1.333279013633728, "TIMESTAMP": 1638760330.3954563}
{"task": "human_sexuality_test", "task_step": 3, "global_step": 34, "loss_val": 1.3260072469711304, "TIMESTAMP": 1638760336.2730157}
{"task": "us_foreign_policy_test", "task_step": 3, "global_step": 35, "loss_val": 1.4243266582489014, "TIMESTAMP": 1638760342.0349498}
{"task": "management_test", "task_step": 1, "global_step": 36, "loss_val": 1.4217182397842407, "TIMESTAMP": 1638760347.8181944}
{"task": "high_school_government_and_politics_test", "task_step": 3, "global_step": 37, "loss_val": 1.3901220560073853, "TIMESTAMP": 1638760353.649381}
{"task": "high_school_chemistry_test", "task_step": 1, "global_step": 38, "loss_val": 1.3905493021011353, "TIMESTAMP": 1638760359.4100287}
{"task": "anatomy_test", "task_step": 2, "global_step": 39, "loss_val": 1.228313684463501, "TIMESTAMP": 1638760365.1731753}
{"task": "college_biology_test", "task_step": 2, "global_step": 40, "loss_val": 1.3542410135269165, "TIMESTAMP": 1638760370.9332511}
{"task": "high_school_microeconomics_test", "task_step": 1, "global_step": 41, "loss_val": 1.4189425706863403, "TIMESTAMP": 1638760376.6887958}
{"task": "high_school_us_history_test", "task_step": 1, "global_step": 42, "loss_val": 1.3787626028060913, "TIMESTAMP": 1638760382.4318662}
{"task": "formal_logic_test", "task_step": 2, "global_step": 43, "loss_val": 1.3551806211471558, "TIMESTAMP": 1638760388.1935136}
{"task": "machine_learning_test", "task_step": 1, "global_step": 44, "loss_val": 1.3828192949295044, "TIMESTAMP": 1638760393.9750388}
{"task": "miscellaneous_test", "task_step": 1, "global_step": 45, "loss_val": 1.4092592000961304, "TIMESTAMP": 1638760399.7312748}
{"task": "philosophy_test", "task_step": 1, "global_step": 46, "loss_val": 1.4361515045166016, "TIMESTAMP": 1638760405.6055427}
{"task": "professional_law_test", "task_step": 2, "global_step": 47, "loss_val": 1.4527106285095215, "TIMESTAMP": 1638760411.3692648}
{"task": "high_school_world_history_test", "task_step": 1, "global_step": 48, "loss_val": 1.4338821172714233, "TIMESTAMP": 1638760417.116757}
{"task": "business_ethics_test", "task_step": 1, "global_step": 49, "loss_val": 1.4105799198150635, "TIMESTAMP": 1638760422.8614516}
{"task": "professional_medicine_test", "task_step": 1, "global_step": 50, "loss_val": 1.3965284824371338, "TIMESTAMP": 1638760428.6058826}
{"task": "moral_scenarios_test", "task_step": 1, "global_step": 51, "loss_val": 1.4184801578521729, "TIMESTAMP": 1638760434.3313038}
{"task": "high_school_world_history_test", "task_step": 2, "global_step": 52, "loss_val": 1.3916542530059814, "TIMESTAMP": 1638760440.0494494}
{"task": "machine_learning_test", "task_step": 2, "global_step": 53, "loss_val": 1.3590284585952759, "TIMESTAMP": 1638760445.7767146}
{"task": "jurisprudence_test", "task_step": 1, "global_step": 54, "loss_val": 1.4150948524475098, "TIMESTAMP": 1638760451.509124}
{"task": "marketing_test", "task_step": 1, "global_step": 55, "loss_val": 1.3854557275772095, "TIMESTAMP": 1638760457.2233033}
{"task": "sociology_test", "task_step": 2, "global_step": 56, "loss_val": 1.3506629467010498, "TIMESTAMP": 1638760463.0773473}
{"task": "clinical_knowledge_test", "task_step": 2, "global_step": 57, "loss_val": 1.1553914546966553, "TIMESTAMP": 1638760468.853944}
{"task": "high_school_macroeconomics_test", "task_step": 2, "global_step": 58, "loss_val": 1.3339673280715942, "TIMESTAMP": 1638760474.6248775}
{"task": "college_computer_science_test", "task_step": 1, "global_step": 59, "loss_val": 1.4291527271270752, "TIMESTAMP": 1638760480.3806906}
{"task": "management_test", "task_step": 2, "global_step": 60, "loss_val": 1.4313571453094482, "TIMESTAMP": 1638760533.2778153}
{"task": "logical_fallacies_test", "task_step": 1, "global_step": 61, "loss_val": 1.377502679824829, "TIMESTAMP": 1638760537.1467445}
{"task": "high_school_computer_science_test", "task_step": 2, "global_step": 62, "loss_val": 1.2386367321014404, "TIMESTAMP": 1638760541.0194104}
{"task": "human_aging_test", "task_step": 1, "global_step": 63, "loss_val": 1.4156767129898071, "TIMESTAMP": 1638760544.8892963}
{"task": "prehistory_test", "task_step": 1, "global_step": 64, "loss_val": 1.451235055923462, "TIMESTAMP": 1638760548.7897236}
{"task": "marketing_test", "task_step": 2, "global_step": 65, "loss_val": 1.3575072288513184, "TIMESTAMP": 1638760552.6811938}
{"task": "high_school_geography_test", "task_step": 1, "global_step": 66, "loss_val": 1.434494972229004, "TIMESTAMP": 1638760556.584085}
{"task": "high_school_physics_test", "task_step": 1, "global_step": 67, "loss_val": 1.4622513055801392, "TIMESTAMP": 1638760560.5161667}
{"task": "college_computer_science_test", "task_step": 2, "global_step": 68, "loss_val": 1.5528404712677002, "TIMESTAMP": 1638760564.424419}
{"task": "human_aging_test", "task_step": 2, "global_step": 69, "loss_val": 1.16826593875885, "TIMESTAMP": 1638760568.384718}
{"task": "moral_scenarios_test", "task_step": 2, "global_step": 70, "loss_val": 1.4433337450027466, "TIMESTAMP": 1638760572.290538}
{"task": "high_school_statistics_test", "task_step": 1, "global_step": 71, "loss_val": 1.4430116415023804, "TIMESTAMP": 1638760576.1889813}
{"task": "econometrics_test", "task_step": 1, "global_step": 72, "loss_val": 1.3436797857284546, "TIMESTAMP": 1638760580.069362}
{"task": "college_medicine_test", "task_step": 1, "global_step": 73, "loss_val": 1.3650703430175781, "TIMESTAMP": 1638760583.9533894}
{"task": "elementary_mathematics_test", "task_step": 2, "global_step": 74, "loss_val": 1.3912183046340942, "TIMESTAMP": 1638760587.8141108}
{"task": "high_school_mathematics_test", "task_step": 2, "global_step": 75, "loss_val": 1.40957510471344, "TIMESTAMP": 1638760591.8087385}
{"task": "high_school_computer_science_test", "task_step": 3, "global_step": 76, "loss_val": 1.0088435411453247, "TIMESTAMP": 1638760595.6654484}
{"task": "moral_disputes_test", "task_step": 1, "global_step": 77, "loss_val": 1.1877830028533936, "TIMESTAMP": 1638760599.519261}
{"task": "econometrics_test", "task_step": 2, "global_step": 78, "loss_val": 1.4107933044433594, "TIMESTAMP": 1638760603.3594174}
{"task": "nutrition_test", "task_step": 1, "global_step": 79, "loss_val": 1.4814389944076538, "TIMESTAMP": 1638760607.2113037}
{"task": "professional_accounting_test", "task_step": 2, "global_step": 80, "loss_val": 1.3769489526748657, "TIMESTAMP": 1638760611.054584}
{"task": "prehistory_test", "task_step": 2, "global_step": 81, "loss_val": 1.3307137489318848, "TIMESTAMP": 1638760614.8704693}
{"task": "nutrition_test", "task_step": 2, "global_step": 82, "loss_val": 1.3735367059707642, "TIMESTAMP": 1638760618.7138388}
{"task": "computer_security_test", "task_step": 1, "global_step": 83, "loss_val": 1.3586870431900024, "TIMESTAMP": 1638760622.5371187}
{"task": "conceptual_physics_test", "task_step": 1, "global_step": 84, "loss_val": 1.3912489414215088, "TIMESTAMP": 1638760626.3983102}
{"task": "world_religions_test", "task_step": 3, "global_step": 85, "loss_val": 1.2930054664611816, "TIMESTAMP": 1638760630.2401154}
{"task": "high_school_statistics_test", "task_step": 2, "global_step": 86, "loss_val": 1.403233528137207, "TIMESTAMP": 1638760634.0892723}
{"task": "moral_disputes_test", "task_step": 2, "global_step": 87, "loss_val": 1.0133577585220337, "TIMESTAMP": 1638760637.9443505}
{"task": "high_school_biology_test", "task_step": 2, "global_step": 88, "loss_val": 1.3996407985687256, "TIMESTAMP": 1638760641.783048}
{"task": "high_school_microeconomics_test", "task_step": 2, "global_step": 89, "loss_val": 1.3780910968780518, "TIMESTAMP": 1638760645.6492603}
{"task": "nutrition_test", "task_step": 3, "global_step": 90, "loss_val": 1.3725934028625488, "TIMESTAMP": 1638760696.1438506}
{"task": "high_school_psychology_test", "task_step": 1, "global_step": 91, "loss_val": 1.4802567958831787, "TIMESTAMP": 1638760700.1654243}
{"task": "college_mathematics_test", "task_step": 2, "global_step": 92, "loss_val": 1.3610084056854248, "TIMESTAMP": 1638760704.2063959}
{"task": "college_medicine_test", "task_step": 2, "global_step": 93, "loss_val": 1.294707179069519, "TIMESTAMP": 1638760708.2489579}
{"task": "high_school_us_history_test", "task_step": 2, "global_step": 94, "loss_val": 1.170559048652649, "TIMESTAMP": 1638760712.2665677}
{"task": "professional_medicine_test", "task_step": 2, "global_step": 95, "loss_val": 1.1648204326629639, "TIMESTAMP": 1638760716.2976441}
{"task": "business_ethics_test", "task_step": 2, "global_step": 96, "loss_val": 1.3708893060684204, "TIMESTAMP": 1638760720.347166}
{"task": "logical_fallacies_test", "task_step": 2, "global_step": 97, "loss_val": 0.980141818523407, "TIMESTAMP": 1638760724.3987856}
{"task": "world_religions_test", "task_step": 4, "global_step": 98, "loss_val": 1.1725223064422607, "TIMESTAMP": 1638760728.4411085}
{"task": "astronomy_test", "task_step": 2, "global_step": 99, "loss_val": 1.4027214050292969, "TIMESTAMP": 1638760732.491877}
{"task": "high_school_psychology_test", "task_step": 2, "global_step": 100, "loss_val": 1.4325840473175049, "TIMESTAMP": 1638760736.5344222}
{"task": "jurisprudence_test", "task_step": 2, "global_step": 101, "loss_val": 1.4541860818862915, "TIMESTAMP": 1638760740.5688121}
{"task": "abstract_algebra_test", "task_step": 3, "global_step": 102, "loss_val": 1.4054536819458008, "TIMESTAMP": 1638760744.6065936}
{"task": "medical_genetics_test", "task_step": 2, "global_step": 103, "loss_val": 1.5895769596099854, "TIMESTAMP": 1638760748.6212423}
{"task": "high_school_physics_test", "task_step": 2, "global_step": 104, "loss_val": 1.411571741104126, "TIMESTAMP": 1638760752.6358745}
{"task": "high_school_chemistry_test", "task_step": 2, "global_step": 105, "loss_val": 1.3404603004455566, "TIMESTAMP": 1638760756.6483846}
{"task": "miscellaneous_test", "task_step": 2, "global_step": 106, "loss_val": 1.3460102081298828, "TIMESTAMP": 1638760760.6796484}
{"task": "high_school_microeconomics_test", "task_step": 3, "global_step": 107, "loss_val": 1.2693383693695068, "TIMESTAMP": 1638760764.6904716}
{"task": "conceptual_physics_test", "task_step": 2, "global_step": 108, "loss_val": 1.2038848400115967, "TIMESTAMP": 1638760768.6919065}
{"task": "high_school_physics_test", "task_step": 3, "global_step": 109, "loss_val": 1.3220428228378296, "TIMESTAMP": 1638760772.8175378}
{"task": "high_school_geography_test", "task_step": 2, "global_step": 110, "loss_val": 1.4415463209152222, "TIMESTAMP": 1638760776.820682}
{"task": "high_school_statistics_test", "task_step": 3, "global_step": 111, "loss_val": 1.384945273399353, "TIMESTAMP": 1638760780.8366606}
{"task": "business_ethics_test", "task_step": 3, "global_step": 112, "loss_val": 1.3268336057662964, "TIMESTAMP": 1638760784.8462152}
{"task": "logical_fallacies_test", "task_step": 3, "global_step": 113, "loss_val": 0.7560297250747681, "TIMESTAMP": 1638760788.825849}
{"task": "high_school_macroeconomics_test", "task_step": 3, "global_step": 114, "loss_val": 1.0868005752563477, "TIMESTAMP": 1638760792.828924}
{"task": "virology_test", "task_step": 1, "global_step": 115, "loss_val": 1.3510870933532715, "TIMESTAMP": 1638760796.8330243}
{"task": "security_studies_test", "task_step": 2, "global_step": 116, "loss_val": 1.316901445388794, "TIMESTAMP": 1638760800.8484402}
{"task": "virology_test", "task_step": 2, "global_step": 117, "loss_val": 1.2676310539245605, "TIMESTAMP": 1638760804.8429973}
{"task": "professional_psychology_test", "task_step": 1, "global_step": 118, "loss_val": 1.4180444478988647, "TIMESTAMP": 1638760808.8386774}
{"task": "college_medicine_test", "task_step": 3, "global_step": 119, "loss_val": 1.3458919525146484, "TIMESTAMP": 1638760812.828251}
{"task": "electrical_engineering_test", "task_step": 2, "global_step": 120, "loss_val": 1.3708713054656982, "TIMESTAMP": 1638760862.3536758}
{"task": "high_school_biology_test", "task_step": 3, "global_step": 121, "loss_val": 1.2683055400848389, "TIMESTAMP": 1638760866.7532313}
{"task": "global_facts_test", "task_step": 1, "global_step": 122, "loss_val": 1.4163180589675903, "TIMESTAMP": 1638760871.154383}
{"task": "clinical_knowledge_test", "task_step": 3, "global_step": 123, "loss_val": 1.1363475322723389, "TIMESTAMP": 1638760875.5647254}
{"task": "public_relations_test", "task_step": 1, "global_step": 124, "loss_val": 1.3787565231323242, "TIMESTAMP": 1638760879.9882143}
{"task": "high_school_chemistry_test", "task_step": 3, "global_step": 125, "loss_val": 1.1791490316390991, "TIMESTAMP": 1638760884.425574}
{"task": "electrical_engineering_test", "task_step": 3, "global_step": 126, "loss_val": 1.2813613414764404, "TIMESTAMP": 1638760888.8563178}
{"task": "professional_psychology_test", "task_step": 2, "global_step": 127, "loss_val": 1.4422670602798462, "TIMESTAMP": 1638760893.2784092}
{"task": "astronomy_test", "task_step": 3, "global_step": 128, "loss_val": 1.3069418668746948, "TIMESTAMP": 1638760897.6660457}
{"task": "professional_psychology_test", "task_step": 3, "global_step": 129, "loss_val": 1.3178188800811768, "TIMESTAMP": 1638760902.0681608}
{"task": "machine_learning_test", "task_step": 3, "global_step": 130, "loss_val": 1.4262259006500244, "TIMESTAMP": 1638760906.445399}
{"task": "marketing_test", "task_step": 3, "global_step": 131, "loss_val": 1.025246024131775, "TIMESTAMP": 1638760910.8411624}
{"task": "professional_accounting_test", "task_step": 3, "global_step": 132, "loss_val": 1.1669114828109741, "TIMESTAMP": 1638760915.2389042}
{"task": "machine_learning_test", "task_step": 4, "global_step": 133, "loss_val": 1.4002685546875, "TIMESTAMP": 1638760919.637547}
{"task": "moral_disputes_test", "task_step": 3, "global_step": 134, "loss_val": 0.7496070861816406, "TIMESTAMP": 1638760924.0434113}
{"task": "international_law_test", "task_step": 2, "global_step": 135, "loss_val": 1.3484597206115723, "TIMESTAMP": 1638760928.450073}
{"task": "human_aging_test", "task_step": 3, "global_step": 136, "loss_val": 1.134610652923584, "TIMESTAMP": 1638760932.8356557}
{"task": "econometrics_test", "task_step": 3, "global_step": 137, "loss_val": 1.4107003211975098, "TIMESTAMP": 1638760937.2371302}
{"task": "high_school_geography_test", "task_step": 3, "global_step": 138, "loss_val": 1.242968201637268, "TIMESTAMP": 1638760941.6532826}
{"task": "philosophy_test", "task_step": 2, "global_step": 139, "loss_val": 0.9663154482841492, "TIMESTAMP": 1638760946.0365367}
{"task": "computer_security_test", "task_step": 2, "global_step": 140, "loss_val": 1.4188522100448608, "TIMESTAMP": 1638760950.442503}
{"task": "public_relations_test", "task_step": 2, "global_step": 141, "loss_val": 1.3812310695648193, "TIMESTAMP": 1638760954.8455524}
{"task": "elementary_mathematics_test", "task_step": 3, "global_step": 142, "loss_val": 1.3417257070541382, "TIMESTAMP": 1638760959.2463882}
{"task": "high_school_psychology_test", "task_step": 3, "global_step": 143, "loss_val": 1.3598684072494507, "TIMESTAMP": 1638760963.6211386}
{"task": "high_school_european_history_test", "task_step": 3, "global_step": 144, "loss_val": 1.3442927598953247, "TIMESTAMP": 1638760968.0160213}
{"task": "professional_medicine_test", "task_step": 3, "global_step": 145, "loss_val": 0.9300214648246765, "TIMESTAMP": 1638760972.4249654}
{"task": "formal_logic_test", "task_step": 3, "global_step": 146, "loss_val": 1.3546793460845947, "TIMESTAMP": 1638760976.821422}
{"task": "sociology_test", "task_step": 3, "global_step": 147, "loss_val": 1.0620818138122559, "TIMESTAMP": 1638760981.194196}
{"task": "econometrics_test", "task_step": 4, "global_step": 148, "loss_val": 1.3215065002441406, "TIMESTAMP": 1638760985.591166}
{"task": "college_physics_test", "task_step": 2, "global_step": 149, "loss_val": 1.2913697957992554, "TIMESTAMP": 1638760990.011245}
{"task": "college_computer_science_test", "task_step": 3, "global_step": 150, "loss_val": 1.4063079357147217, "TIMESTAMP": 1638761040.1152472}
{"task": "high_school_microeconomics_test", "task_step": 4, "global_step": 151, "loss_val": 1.3218560218811035, "TIMESTAMP": 1638761044.8986568}
{"task": "high_school_us_history_test", "task_step": 3, "global_step": 152, "loss_val": 1.0564817190170288, "TIMESTAMP": 1638761049.6474478}
{"task": "public_relations_test", "task_step": 3, "global_step": 153, "loss_val": 1.3731778860092163, "TIMESTAMP": 1638761054.443486}
{"task": "professional_accounting_test", "task_step": 4, "global_step": 154, "loss_val": 1.2233760356903076, "TIMESTAMP": 1638761059.2548835}
{"task": "professional_psychology_test", "task_step": 4, "global_step": 155, "loss_val": 1.3212294578552246, "TIMESTAMP": 1638761064.034944}
{"task": "moral_scenarios_test", "task_step": 3, "global_step": 156, "loss_val": 1.2927993535995483, "TIMESTAMP": 1638761068.8115842}
{"task": "miscellaneous_test", "task_step": 3, "global_step": 157, "loss_val": 1.3826345205307007, "TIMESTAMP": 1638761073.5986366}
{"task": "international_law_test", "task_step": 3, "global_step": 158, "loss_val": 1.3746763467788696, "TIMESTAMP": 1638761078.4582224}
{"task": "international_law_test", "task_step": 4, "global_step": 159, "loss_val": 1.3537461757659912, "TIMESTAMP": 1638761083.229685}
{"task": "college_chemistry_test", "task_step": 3, "global_step": 160, "loss_val": 1.3909398317337036, "TIMESTAMP": 1638761087.9881797}
{"task": "global_facts_test", "task_step": 2, "global_step": 161, "loss_val": 1.4170557260513306, "TIMESTAMP": 1638761092.7679052}
{"task": "philosophy_test", "task_step": 3, "global_step": 162, "loss_val": 0.7881113886833191, "TIMESTAMP": 1638761097.531812}
{"task": "management_test", "task_step": 3, "global_step": 163, "loss_val": 1.4288593530654907, "TIMESTAMP": 1638761102.2951849}
{"task": "prehistory_test", "task_step": 3, "global_step": 164, "loss_val": 1.3095260858535767, "TIMESTAMP": 1638761107.071937}
{"task": "high_school_mathematics_test", "task_step": 3, "global_step": 165, "loss_val": 1.3745129108428955, "TIMESTAMP": 1638761111.860917}
{"task": "security_studies_test", "task_step": 3, "global_step": 166, "loss_val": 1.2928519248962402, "TIMESTAMP": 1638761116.774339}
{"task": "college_biology_test", "task_step": 3, "global_step": 167, "loss_val": 1.3092491626739502, "TIMESTAMP": 1638761121.6090314}
{"task": "miscellaneous_test", "task_step": 4, "global_step": 168, "loss_val": 1.1802797317504883, "TIMESTAMP": 1638761126.3761146}
{"task": "management_test", "task_step": 4, "global_step": 169, "loss_val": 1.4182331562042236, "TIMESTAMP": 1638761131.1652358}
{"task": "professional_psychology_test", "task_step": 5, "global_step": 170, "loss_val": 1.3876863718032837, "TIMESTAMP": 1638761135.9461412}
{"task": "medical_genetics_test", "task_step": 3, "global_step": 171, "loss_val": 1.4159443378448486, "TIMESTAMP": 1638761140.7115834}
